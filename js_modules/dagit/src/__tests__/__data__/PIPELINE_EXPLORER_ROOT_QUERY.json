{"data": {"pipelineOrError": {"description": null, "modes": [{"description": null, "loggers": [{"configField": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": null, "isOptional": true, "name": "log_level"}, {"configType": {"key": "String"}, "description": null, "isOptional": true, "name": "name"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.25", "name": null}}, "description": "The default colored console logger.", "name": "console"}], "name": "local", "resources": [{"configField": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "postgres_db_name"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "postgres_hostname"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "postgres_password"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "postgres_username"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.49", "name": null}}, "description": null, "name": "db_info"}, {"configField": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "Bool"}, "description": null, "isOptional": true, "name": "overwrite"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "target_folder"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}, {"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "Bool", "name": "Bool"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.41", "name": null}}, "description": null, "name": "file_cache"}, {"configField": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "Bool"}, "description": "Specifies whether to use an unsigned S3 session", "isOptional": true, "name": "use_unsigned_session"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "Bool", "name": "Bool"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.43", "name": null}}, "description": null, "name": "s3"}, {"configField": null, "description": null, "name": "spark"}, {"configField": null, "description": null, "name": "tempfile"}]}, {"description": null, "loggers": [{"configField": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": null, "isOptional": true, "name": "log_level"}, {"configType": {"key": "String"}, "description": null, "isOptional": true, "name": "name"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.25", "name": null}}, "description": "The default colored console logger.", "name": "console"}], "name": "prod", "resources": [{"configField": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "redshift_db_name"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "redshift_hostname"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "redshift_password"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "redshift_username"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "s3_temp_dir"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.48", "name": null}}, "description": null, "name": "db_info"}, {"configField": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "bucket"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "key"}, {"configType": {"key": "Bool"}, "description": null, "isOptional": true, "name": "overwrite"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}, {"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "Bool", "name": "Bool"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.42", "name": null}}, "description": null, "name": "file_cache"}, {"configField": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "Bool"}, "description": "Specifies whether to use an unsigned S3 session", "isOptional": true, "name": "use_unsigned_session"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "Bool", "name": "Bool"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.43", "name": null}}, "description": null, "name": "s3"}, {"configField": null, "description": null, "name": "spark"}, {"configField": null, "description": null, "name": "tempfile"}]}, {"description": null, "loggers": [{"configField": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": null, "isOptional": true, "name": "log_level"}, {"configType": {"key": "String"}, "description": null, "isOptional": true, "name": "name"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.25", "name": null}}, "description": "The default colored console logger.", "name": "console"}], "name": "test", "resources": [{"configField": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "redshift_db_name"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "redshift_hostname"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "redshift_password"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "redshift_username"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "s3_temp_dir"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.48", "name": null}}, "description": null, "name": "db_info"}, {"configField": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "Bool"}, "description": null, "isOptional": true, "name": "overwrite"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "target_folder"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}, {"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "Bool", "name": "Bool"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.41", "name": null}}, "description": null, "name": "file_cache"}, {"configField": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "Bool"}, "description": "Specifies whether to use an unsigned S3 session", "isOptional": true, "name": "use_unsigned_session"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "Bool", "name": "Bool"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.43", "name": null}}, "description": null, "name": "s3"}, {"configField": null, "description": null, "name": "spark"}, {"configField": null, "description": null, "name": "tempfile"}]}], "name": "airline_demo_ingest_pipeline", "solidHandles": [{"handleID": "april_on_time_s3_to_df", "parent": null, "solid": {"definition": {"__typename": "CompositeSolidDefinition", "description": "Ingest a zipped csv file from s3,\nstash in a keyed file store (does not download if already\npresent by default), unzip that file, and load it into a\nSpark Dataframe. See documentation in constituent solids for\nmore detail.", "inputDefinitions": [{"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}], "inputMappings": [{"definition": {"name": "s3_coordinate"}, "mappedInput": {"definition": {"name": "s3_coordinate"}, "solid": {"name": "cache_file_from_s3"}}}, {"definition": {"name": "archive_member"}, "mappedInput": {"definition": {"name": "archive_member"}, "solid": {"name": "unzip_file_handle"}}}], "metadata": [], "name": "s3_to_df", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "outputMappings": [{"definition": {"name": "result"}, "mappedOutput": {"definition": {"name": "result"}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}}], "requiredResources": [{"resourceKey": "s3"}, {"resourceKey": "spark"}, {"resourceKey": "file_cache"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "april_on_time_s3_to_df", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependedBy": [{"definition": {"name": "april_data", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "join_q2_data"}}]}]}}, {"handleID": "april_on_time_s3_to_df.cache_file_from_s3", "parent": {"handleID": "april_on_time_s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": "Optionally specify the key for the file to be ingested into the keyed store. Defaults to the last path component of the downloaded s3 key.", "isOptional": true, "name": "file_key"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.47", "name": null}}, "description": "This is a solid which caches a file in s3 into file cache.\n\nThe `file_cache` is a resource type that allows a solid author to save files\nand assign a key to them. The keyed file store can be backed by local file or any\nobject store (currently we support s3). This keyed file store can be configured\nto be at an external location so that is persists in a well known spot between runs.\nIt is designed for the case where there is an expensive download step that should not\noccur unless the downloaded file does not exist. Redownload can be instigated either\nby configuring the source to overwrite files or to just delete the file in the underlying\nstorage manually.\n\nThis works by downloading the file to a temporary file, and then ingesting it into\nthe file cache. In the case of a filesystem-backed file cache, this is a file\ncopy. In the case of a object-store-backed file cache, this is an upload.\n\nIn order to work this must be executed within a mode that provides an `s3`\nand `file_cache` resource.\n    ", "inputDefinitions": [{"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}], "metadata": [], "name": "cache_file_from_s3", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}], "requiredResources": [{"resourceKey": "s3"}, {"resourceKey": "file_cache"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}], "name": "cache_file_from_s3", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "archive_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}]}}, {"handleID": "april_on_time_s3_to_df.ingest_csv_file_handle_to_spark", "parent": {"handleID": "april_on_time_s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Take a file handle that contains a csv with headers and load it\ninto a Spark DataFrame. It infers header names but does *not* infer schema.\n\nIt also ensures that the column names are valid parquet column names by\nfiltering out any of the following characters from column names:\n\nCharacters (within quotations): \"`[ ,;{}()\\n\\t=]`\"\n\n", "inputDefinitions": [{"description": null, "name": "csv_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}], "metadata": [], "name": "ingest_csv_file_handle_to_spark", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "requiredResources": [{"resourceKey": "spark"}]}, "inputs": [{"definition": {"description": null, "name": "csv_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}], "name": "ingest_csv_file_handle_to_spark", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependedBy": []}]}}, {"handleID": "april_on_time_s3_to_df.unzip_file_handle", "parent": {"handleID": "april_on_time_s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Unzip a file that is resident in an archive file as a member.\n    This solid operates on FileHandles, meaning that their physical is dependent\n    on what system storage is operating in the pipeline. The physical file could\n    be on local disk, or it could be in s3. If on s3, this solid will download\n    that file to local disk, perform the unzip, upload that file back to s3, and\n    then return that file handle for downstream use in the computations.\n    ", "inputDefinitions": [{"description": null, "name": "archive_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}], "metadata": [], "name": "unzip_file_handle", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}], "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "archive_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "cache_file_from_s3"}}]}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "unzip_file_handle", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "csv_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}]}]}}, {"handleID": "download_q2_sfo_weather", "parent": null, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": "Optionally specify the key for the file to be ingested into the keyed store. Defaults to the last path component of the downloaded s3 key.", "isOptional": true, "name": "file_key"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.47", "name": null}}, "description": "This is a solid which caches a file in s3 into file cache.\n\nThe `file_cache` is a resource type that allows a solid author to save files\nand assign a key to them. The keyed file store can be backed by local file or any\nobject store (currently we support s3). This keyed file store can be configured\nto be at an external location so that is persists in a well known spot between runs.\nIt is designed for the case where there is an expensive download step that should not\noccur unless the downloaded file does not exist. Redownload can be instigated either\nby configuring the source to overwrite files or to just delete the file in the underlying\nstorage manually.\n\nThis works by downloading the file to a temporary file, and then ingesting it into\nthe file cache. In the case of a filesystem-backed file cache, this is a file\ncopy. In the case of a object-store-backed file cache, this is an upload.\n\nIn order to work this must be executed within a mode that provides an `s3`\nand `file_cache` resource.\n    ", "inputDefinitions": [{"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}], "metadata": [], "name": "cache_file_from_s3", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}], "requiredResources": [{"resourceKey": "s3"}, {"resourceKey": "file_cache"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}], "name": "download_q2_sfo_weather", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "csv_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "ingest_q2_sfo_weather"}}]}]}}, {"handleID": "ingest_q2_sfo_weather", "parent": null, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Take a file handle that contains a csv with headers and load it\ninto a Spark DataFrame. It infers header names but does *not* infer schema.\n\nIt also ensures that the column names are valid parquet column names by\nfiltering out any of the following characters from column names:\n\nCharacters (within quotations): \"`[ ,;{}()\\n\\t=]`\"\n\n", "inputDefinitions": [{"description": null, "name": "csv_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}], "metadata": [], "name": "ingest_csv_file_handle_to_spark", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "requiredResources": [{"resourceKey": "spark"}]}, "inputs": [{"definition": {"description": null, "name": "csv_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "download_q2_sfo_weather"}}]}], "name": "ingest_q2_sfo_weather", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependedBy": [{"definition": {"name": "sfo_weather_data", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "process_sfo_weather_data"}}]}]}}, {"handleID": "join_q2_data", "parent": null, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "Int"}, "description": null, "isOptional": false, "name": "subsample_pct"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "Int", "name": "Int"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.168", "name": null}}, "description": "\n    This solid takes April, May, and June data and coalesces it into a q2 data set.\n    It then joins the that origin and destination airport with the data in the\n    master_cord_data.\n    ", "inputDefinitions": [{"description": null, "name": "april_data", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, {"description": null, "name": "may_data", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, {"description": null, "name": "june_data", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, {"description": null, "name": "master_cord_data", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "metadata": [], "name": "join_q2_data", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "april_data", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "april_on_time_s3_to_df"}}]}, {"definition": {"description": null, "name": "may_data", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "may_on_time_s3_to_df"}}]}, {"definition": {"description": null, "name": "june_data", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "june_on_time_s3_to_df"}}]}, {"definition": {"description": null, "name": "master_cord_data", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "master_cord_s3_to_df"}}]}], "name": "join_q2_data", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependedBy": [{"definition": {"name": "data_frame", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "load_q2_on_time_data"}}]}]}}, {"handleID": "june_on_time_s3_to_df", "parent": null, "solid": {"definition": {"__typename": "CompositeSolidDefinition", "description": "Ingest a zipped csv file from s3,\nstash in a keyed file store (does not download if already\npresent by default), unzip that file, and load it into a\nSpark Dataframe. See documentation in constituent solids for\nmore detail.", "inputDefinitions": [{"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}], "inputMappings": [{"definition": {"name": "s3_coordinate"}, "mappedInput": {"definition": {"name": "s3_coordinate"}, "solid": {"name": "cache_file_from_s3"}}}, {"definition": {"name": "archive_member"}, "mappedInput": {"definition": {"name": "archive_member"}, "solid": {"name": "unzip_file_handle"}}}], "metadata": [], "name": "s3_to_df", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "outputMappings": [{"definition": {"name": "result"}, "mappedOutput": {"definition": {"name": "result"}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}}], "requiredResources": [{"resourceKey": "s3"}, {"resourceKey": "spark"}, {"resourceKey": "file_cache"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "june_on_time_s3_to_df", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependedBy": [{"definition": {"name": "june_data", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "join_q2_data"}}]}]}}, {"handleID": "june_on_time_s3_to_df.cache_file_from_s3", "parent": {"handleID": "june_on_time_s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": "Optionally specify the key for the file to be ingested into the keyed store. Defaults to the last path component of the downloaded s3 key.", "isOptional": true, "name": "file_key"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.47", "name": null}}, "description": "This is a solid which caches a file in s3 into file cache.\n\nThe `file_cache` is a resource type that allows a solid author to save files\nand assign a key to them. The keyed file store can be backed by local file or any\nobject store (currently we support s3). This keyed file store can be configured\nto be at an external location so that is persists in a well known spot between runs.\nIt is designed for the case where there is an expensive download step that should not\noccur unless the downloaded file does not exist. Redownload can be instigated either\nby configuring the source to overwrite files or to just delete the file in the underlying\nstorage manually.\n\nThis works by downloading the file to a temporary file, and then ingesting it into\nthe file cache. In the case of a filesystem-backed file cache, this is a file\ncopy. In the case of a object-store-backed file cache, this is an upload.\n\nIn order to work this must be executed within a mode that provides an `s3`\nand `file_cache` resource.\n    ", "inputDefinitions": [{"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}], "metadata": [], "name": "cache_file_from_s3", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}], "requiredResources": [{"resourceKey": "s3"}, {"resourceKey": "file_cache"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}], "name": "cache_file_from_s3", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "archive_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}]}}, {"handleID": "june_on_time_s3_to_df.ingest_csv_file_handle_to_spark", "parent": {"handleID": "june_on_time_s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Take a file handle that contains a csv with headers and load it\ninto a Spark DataFrame. It infers header names but does *not* infer schema.\n\nIt also ensures that the column names are valid parquet column names by\nfiltering out any of the following characters from column names:\n\nCharacters (within quotations): \"`[ ,;{}()\\n\\t=]`\"\n\n", "inputDefinitions": [{"description": null, "name": "csv_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}], "metadata": [], "name": "ingest_csv_file_handle_to_spark", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "requiredResources": [{"resourceKey": "spark"}]}, "inputs": [{"definition": {"description": null, "name": "csv_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}], "name": "ingest_csv_file_handle_to_spark", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependedBy": []}]}}, {"handleID": "june_on_time_s3_to_df.unzip_file_handle", "parent": {"handleID": "june_on_time_s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Unzip a file that is resident in an archive file as a member.\n    This solid operates on FileHandles, meaning that their physical is dependent\n    on what system storage is operating in the pipeline. The physical file could\n    be on local disk, or it could be in s3. If on s3, this solid will download\n    that file to local disk, perform the unzip, upload that file back to s3, and\n    then return that file handle for downstream use in the computations.\n    ", "inputDefinitions": [{"description": null, "name": "archive_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}], "metadata": [], "name": "unzip_file_handle", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}], "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "archive_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "cache_file_from_s3"}}]}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "unzip_file_handle", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "csv_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}]}]}}, {"handleID": "load_q2_on_time_data", "parent": null, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": "", "isOptional": false, "name": "table_name"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.165", "name": null}}, "description": null, "inputDefinitions": [{"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "metadata": [], "name": "load_data_to_database_from_spark", "outputDefinitions": [{"description": null, "name": "table_name", "type": {"description": null, "displayName": "String", "name": "String"}}], "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "join_q2_data"}}]}], "name": "load_q2_on_time_data", "outputs": [{"definition": {"description": null, "name": "table_name", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependedBy": []}]}}, {"handleID": "load_q2_sfo_weather", "parent": null, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": "", "isOptional": false, "name": "table_name"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.165", "name": null}}, "description": null, "inputDefinitions": [{"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "metadata": [], "name": "load_data_to_database_from_spark", "outputDefinitions": [{"description": null, "name": "table_name", "type": {"description": null, "displayName": "String", "name": "String"}}], "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "process_sfo_weather_data"}}]}], "name": "load_q2_sfo_weather", "outputs": [{"definition": {"description": null, "name": "table_name", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependedBy": []}]}}, {"handleID": "master_cord_s3_to_df", "parent": null, "solid": {"definition": {"__typename": "CompositeSolidDefinition", "description": "Ingest a zipped csv file from s3,\nstash in a keyed file store (does not download if already\npresent by default), unzip that file, and load it into a\nSpark Dataframe. See documentation in constituent solids for\nmore detail.", "inputDefinitions": [{"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}], "inputMappings": [{"definition": {"name": "s3_coordinate"}, "mappedInput": {"definition": {"name": "s3_coordinate"}, "solid": {"name": "cache_file_from_s3"}}}, {"definition": {"name": "archive_member"}, "mappedInput": {"definition": {"name": "archive_member"}, "solid": {"name": "unzip_file_handle"}}}], "metadata": [], "name": "s3_to_df", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "outputMappings": [{"definition": {"name": "result"}, "mappedOutput": {"definition": {"name": "result"}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}}], "requiredResources": [{"resourceKey": "s3"}, {"resourceKey": "spark"}, {"resourceKey": "file_cache"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "master_cord_s3_to_df", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependedBy": [{"definition": {"name": "master_cord_data", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "join_q2_data"}}]}]}}, {"handleID": "master_cord_s3_to_df.cache_file_from_s3", "parent": {"handleID": "master_cord_s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": "Optionally specify the key for the file to be ingested into the keyed store. Defaults to the last path component of the downloaded s3 key.", "isOptional": true, "name": "file_key"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.47", "name": null}}, "description": "This is a solid which caches a file in s3 into file cache.\n\nThe `file_cache` is a resource type that allows a solid author to save files\nand assign a key to them. The keyed file store can be backed by local file or any\nobject store (currently we support s3). This keyed file store can be configured\nto be at an external location so that is persists in a well known spot between runs.\nIt is designed for the case where there is an expensive download step that should not\noccur unless the downloaded file does not exist. Redownload can be instigated either\nby configuring the source to overwrite files or to just delete the file in the underlying\nstorage manually.\n\nThis works by downloading the file to a temporary file, and then ingesting it into\nthe file cache. In the case of a filesystem-backed file cache, this is a file\ncopy. In the case of a object-store-backed file cache, this is an upload.\n\nIn order to work this must be executed within a mode that provides an `s3`\nand `file_cache` resource.\n    ", "inputDefinitions": [{"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}], "metadata": [], "name": "cache_file_from_s3", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}], "requiredResources": [{"resourceKey": "s3"}, {"resourceKey": "file_cache"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}], "name": "cache_file_from_s3", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "archive_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}]}}, {"handleID": "master_cord_s3_to_df.ingest_csv_file_handle_to_spark", "parent": {"handleID": "master_cord_s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Take a file handle that contains a csv with headers and load it\ninto a Spark DataFrame. It infers header names but does *not* infer schema.\n\nIt also ensures that the column names are valid parquet column names by\nfiltering out any of the following characters from column names:\n\nCharacters (within quotations): \"`[ ,;{}()\\n\\t=]`\"\n\n", "inputDefinitions": [{"description": null, "name": "csv_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}], "metadata": [], "name": "ingest_csv_file_handle_to_spark", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "requiredResources": [{"resourceKey": "spark"}]}, "inputs": [{"definition": {"description": null, "name": "csv_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}], "name": "ingest_csv_file_handle_to_spark", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependedBy": []}]}}, {"handleID": "master_cord_s3_to_df.unzip_file_handle", "parent": {"handleID": "master_cord_s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Unzip a file that is resident in an archive file as a member.\n    This solid operates on FileHandles, meaning that their physical is dependent\n    on what system storage is operating in the pipeline. The physical file could\n    be on local disk, or it could be in s3. If on s3, this solid will download\n    that file to local disk, perform the unzip, upload that file back to s3, and\n    then return that file handle for downstream use in the computations.\n    ", "inputDefinitions": [{"description": null, "name": "archive_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}], "metadata": [], "name": "unzip_file_handle", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}], "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "archive_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "cache_file_from_s3"}}]}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "unzip_file_handle", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "csv_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}]}]}}, {"handleID": "may_on_time_s3_to_df", "parent": null, "solid": {"definition": {"__typename": "CompositeSolidDefinition", "description": "Ingest a zipped csv file from s3,\nstash in a keyed file store (does not download if already\npresent by default), unzip that file, and load it into a\nSpark Dataframe. See documentation in constituent solids for\nmore detail.", "inputDefinitions": [{"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}], "inputMappings": [{"definition": {"name": "s3_coordinate"}, "mappedInput": {"definition": {"name": "s3_coordinate"}, "solid": {"name": "cache_file_from_s3"}}}, {"definition": {"name": "archive_member"}, "mappedInput": {"definition": {"name": "archive_member"}, "solid": {"name": "unzip_file_handle"}}}], "metadata": [], "name": "s3_to_df", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "outputMappings": [{"definition": {"name": "result"}, "mappedOutput": {"definition": {"name": "result"}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}}], "requiredResources": [{"resourceKey": "s3"}, {"resourceKey": "spark"}, {"resourceKey": "file_cache"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "may_on_time_s3_to_df", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependedBy": [{"definition": {"name": "may_data", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "join_q2_data"}}]}]}}, {"handleID": "may_on_time_s3_to_df.cache_file_from_s3", "parent": {"handleID": "may_on_time_s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": "Optionally specify the key for the file to be ingested into the keyed store. Defaults to the last path component of the downloaded s3 key.", "isOptional": true, "name": "file_key"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.47", "name": null}}, "description": "This is a solid which caches a file in s3 into file cache.\n\nThe `file_cache` is a resource type that allows a solid author to save files\nand assign a key to them. The keyed file store can be backed by local file or any\nobject store (currently we support s3). This keyed file store can be configured\nto be at an external location so that is persists in a well known spot between runs.\nIt is designed for the case where there is an expensive download step that should not\noccur unless the downloaded file does not exist. Redownload can be instigated either\nby configuring the source to overwrite files or to just delete the file in the underlying\nstorage manually.\n\nThis works by downloading the file to a temporary file, and then ingesting it into\nthe file cache. In the case of a filesystem-backed file cache, this is a file\ncopy. In the case of a object-store-backed file cache, this is an upload.\n\nIn order to work this must be executed within a mode that provides an `s3`\nand `file_cache` resource.\n    ", "inputDefinitions": [{"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}], "metadata": [], "name": "cache_file_from_s3", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}], "requiredResources": [{"resourceKey": "s3"}, {"resourceKey": "file_cache"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}], "name": "cache_file_from_s3", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "archive_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}]}}, {"handleID": "may_on_time_s3_to_df.ingest_csv_file_handle_to_spark", "parent": {"handleID": "may_on_time_s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Take a file handle that contains a csv with headers and load it\ninto a Spark DataFrame. It infers header names but does *not* infer schema.\n\nIt also ensures that the column names are valid parquet column names by\nfiltering out any of the following characters from column names:\n\nCharacters (within quotations): \"`[ ,;{}()\\n\\t=]`\"\n\n", "inputDefinitions": [{"description": null, "name": "csv_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}], "metadata": [], "name": "ingest_csv_file_handle_to_spark", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "requiredResources": [{"resourceKey": "spark"}]}, "inputs": [{"definition": {"description": null, "name": "csv_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}], "name": "ingest_csv_file_handle_to_spark", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependedBy": []}]}}, {"handleID": "may_on_time_s3_to_df.unzip_file_handle", "parent": {"handleID": "may_on_time_s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Unzip a file that is resident in an archive file as a member.\n    This solid operates on FileHandles, meaning that their physical is dependent\n    on what system storage is operating in the pipeline. The physical file could\n    be on local disk, or it could be in s3. If on s3, this solid will download\n    that file to local disk, perform the unzip, upload that file back to s3, and\n    then return that file handle for downstream use in the computations.\n    ", "inputDefinitions": [{"description": null, "name": "archive_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}], "metadata": [], "name": "unzip_file_handle", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}], "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "archive_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "cache_file_from_s3"}}]}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "unzip_file_handle", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "csv_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}]}]}}, {"handleID": "process_q2_coupon_data", "parent": null, "solid": {"definition": {"__typename": "CompositeSolidDefinition", "description": "Ingest zipped csv file from s3, load into a Spark\nDataFrame, optionally subsample it (via configuring the\nsubsample_spark_dataset, solid), canonicalize the column names, and then\nload it into a data warehouse.\n", "inputDefinitions": [{"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}], "inputMappings": [{"definition": {"name": "s3_coordinate"}, "mappedInput": {"definition": {"name": "s3_coordinate"}, "solid": {"name": "s3_to_df"}}}, {"definition": {"name": "archive_member"}, "mappedInput": {"definition": {"name": "archive_member"}, "solid": {"name": "s3_to_df"}}}], "metadata": [], "name": "s3_to_dw_table", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": null, "displayName": "String", "name": "String"}}], "outputMappings": [{"definition": {"name": "result"}, "mappedOutput": {"definition": {"name": "table_name"}, "solid": {"name": "load_data_to_database_from_spark"}}}], "requiredResources": [{"resourceKey": "s3"}, {"resourceKey": "spark"}, {"resourceKey": "file_cache"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "process_q2_coupon_data", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependedBy": []}]}}, {"handleID": "process_q2_coupon_data.canonicalize_column_names", "parent": {"handleID": "process_q2_coupon_data"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": null, "inputDefinitions": [{"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "metadata": [], "name": "canonicalize_column_names", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "subsample_spark_dataset"}}]}], "name": "canonicalize_column_names", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependedBy": [{"definition": {"name": "data_frame", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "load_data_to_database_from_spark"}}]}]}}, {"handleID": "process_q2_coupon_data.load_data_to_database_from_spark", "parent": {"handleID": "process_q2_coupon_data"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": "", "isOptional": false, "name": "table_name"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.165", "name": null}}, "description": null, "inputDefinitions": [{"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "metadata": [], "name": "load_data_to_database_from_spark", "outputDefinitions": [{"description": null, "name": "table_name", "type": {"description": null, "displayName": "String", "name": "String"}}], "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "canonicalize_column_names"}}]}], "name": "load_data_to_database_from_spark", "outputs": [{"definition": {"description": null, "name": "table_name", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependedBy": []}]}}, {"handleID": "process_q2_coupon_data.s3_to_df", "parent": {"handleID": "process_q2_coupon_data"}, "solid": {"definition": {"__typename": "CompositeSolidDefinition", "description": "Ingest a zipped csv file from s3,\nstash in a keyed file store (does not download if already\npresent by default), unzip that file, and load it into a\nSpark Dataframe. See documentation in constituent solids for\nmore detail.", "inputDefinitions": [{"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}], "inputMappings": [{"definition": {"name": "s3_coordinate"}, "mappedInput": {"definition": {"name": "s3_coordinate"}, "solid": {"name": "cache_file_from_s3"}}}, {"definition": {"name": "archive_member"}, "mappedInput": {"definition": {"name": "archive_member"}, "solid": {"name": "unzip_file_handle"}}}], "metadata": [], "name": "s3_to_df", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "outputMappings": [{"definition": {"name": "result"}, "mappedOutput": {"definition": {"name": "result"}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}}], "requiredResources": [{"resourceKey": "s3"}, {"resourceKey": "spark"}, {"resourceKey": "file_cache"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "s3_to_df", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependedBy": [{"definition": {"name": "data_frame", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "subsample_spark_dataset"}}]}]}}, {"handleID": "process_q2_coupon_data.s3_to_df.cache_file_from_s3", "parent": {"handleID": "process_q2_coupon_data.s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": "Optionally specify the key for the file to be ingested into the keyed store. Defaults to the last path component of the downloaded s3 key.", "isOptional": true, "name": "file_key"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.47", "name": null}}, "description": "This is a solid which caches a file in s3 into file cache.\n\nThe `file_cache` is a resource type that allows a solid author to save files\nand assign a key to them. The keyed file store can be backed by local file or any\nobject store (currently we support s3). This keyed file store can be configured\nto be at an external location so that is persists in a well known spot between runs.\nIt is designed for the case where there is an expensive download step that should not\noccur unless the downloaded file does not exist. Redownload can be instigated either\nby configuring the source to overwrite files or to just delete the file in the underlying\nstorage manually.\n\nThis works by downloading the file to a temporary file, and then ingesting it into\nthe file cache. In the case of a filesystem-backed file cache, this is a file\ncopy. In the case of a object-store-backed file cache, this is an upload.\n\nIn order to work this must be executed within a mode that provides an `s3`\nand `file_cache` resource.\n    ", "inputDefinitions": [{"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}], "metadata": [], "name": "cache_file_from_s3", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}], "requiredResources": [{"resourceKey": "s3"}, {"resourceKey": "file_cache"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}], "name": "cache_file_from_s3", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "archive_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}]}}, {"handleID": "process_q2_coupon_data.s3_to_df.ingest_csv_file_handle_to_spark", "parent": {"handleID": "process_q2_coupon_data.s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Take a file handle that contains a csv with headers and load it\ninto a Spark DataFrame. It infers header names but does *not* infer schema.\n\nIt also ensures that the column names are valid parquet column names by\nfiltering out any of the following characters from column names:\n\nCharacters (within quotations): \"`[ ,;{}()\\n\\t=]`\"\n\n", "inputDefinitions": [{"description": null, "name": "csv_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}], "metadata": [], "name": "ingest_csv_file_handle_to_spark", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "requiredResources": [{"resourceKey": "spark"}]}, "inputs": [{"definition": {"description": null, "name": "csv_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}], "name": "ingest_csv_file_handle_to_spark", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependedBy": []}]}}, {"handleID": "process_q2_coupon_data.s3_to_df.unzip_file_handle", "parent": {"handleID": "process_q2_coupon_data.s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Unzip a file that is resident in an archive file as a member.\n    This solid operates on FileHandles, meaning that their physical is dependent\n    on what system storage is operating in the pipeline. The physical file could\n    be on local disk, or it could be in s3. If on s3, this solid will download\n    that file to local disk, perform the unzip, upload that file back to s3, and\n    then return that file handle for downstream use in the computations.\n    ", "inputDefinitions": [{"description": null, "name": "archive_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}], "metadata": [], "name": "unzip_file_handle", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}], "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "archive_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "cache_file_from_s3"}}]}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "unzip_file_handle", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "csv_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}]}]}}, {"handleID": "process_q2_coupon_data.subsample_spark_dataset", "parent": {"handleID": "process_q2_coupon_data"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "Int"}, "description": "The integer percentage of rows to sample from the input dataset.", "isOptional": false, "name": "subsample_pct"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "Int", "name": "Int"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.166", "name": null}}, "description": "Subsample a spark dataset via the configuration option.", "inputDefinitions": [{"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "metadata": [], "name": "subsample_spark_dataset", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "s3_to_df"}}]}], "name": "subsample_spark_dataset", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependedBy": [{"definition": {"name": "data_frame", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "canonicalize_column_names"}}]}]}}, {"handleID": "process_q2_market_data", "parent": null, "solid": {"definition": {"__typename": "CompositeSolidDefinition", "description": "Ingest zipped csv file from s3, load into a Spark\nDataFrame, optionally subsample it (via configuring the\nsubsample_spark_dataset, solid), canonicalize the column names, and then\nload it into a data warehouse.\n", "inputDefinitions": [{"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}], "inputMappings": [{"definition": {"name": "s3_coordinate"}, "mappedInput": {"definition": {"name": "s3_coordinate"}, "solid": {"name": "s3_to_df"}}}, {"definition": {"name": "archive_member"}, "mappedInput": {"definition": {"name": "archive_member"}, "solid": {"name": "s3_to_df"}}}], "metadata": [], "name": "s3_to_dw_table", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": null, "displayName": "String", "name": "String"}}], "outputMappings": [{"definition": {"name": "result"}, "mappedOutput": {"definition": {"name": "table_name"}, "solid": {"name": "load_data_to_database_from_spark"}}}], "requiredResources": [{"resourceKey": "s3"}, {"resourceKey": "spark"}, {"resourceKey": "file_cache"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "process_q2_market_data", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependedBy": []}]}}, {"handleID": "process_q2_market_data.canonicalize_column_names", "parent": {"handleID": "process_q2_market_data"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": null, "inputDefinitions": [{"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "metadata": [], "name": "canonicalize_column_names", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "subsample_spark_dataset"}}]}], "name": "canonicalize_column_names", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependedBy": [{"definition": {"name": "data_frame", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "load_data_to_database_from_spark"}}]}]}}, {"handleID": "process_q2_market_data.load_data_to_database_from_spark", "parent": {"handleID": "process_q2_market_data"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": "", "isOptional": false, "name": "table_name"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.165", "name": null}}, "description": null, "inputDefinitions": [{"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "metadata": [], "name": "load_data_to_database_from_spark", "outputDefinitions": [{"description": null, "name": "table_name", "type": {"description": null, "displayName": "String", "name": "String"}}], "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "canonicalize_column_names"}}]}], "name": "load_data_to_database_from_spark", "outputs": [{"definition": {"description": null, "name": "table_name", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependedBy": []}]}}, {"handleID": "process_q2_market_data.s3_to_df", "parent": {"handleID": "process_q2_market_data"}, "solid": {"definition": {"__typename": "CompositeSolidDefinition", "description": "Ingest a zipped csv file from s3,\nstash in a keyed file store (does not download if already\npresent by default), unzip that file, and load it into a\nSpark Dataframe. See documentation in constituent solids for\nmore detail.", "inputDefinitions": [{"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}], "inputMappings": [{"definition": {"name": "s3_coordinate"}, "mappedInput": {"definition": {"name": "s3_coordinate"}, "solid": {"name": "cache_file_from_s3"}}}, {"definition": {"name": "archive_member"}, "mappedInput": {"definition": {"name": "archive_member"}, "solid": {"name": "unzip_file_handle"}}}], "metadata": [], "name": "s3_to_df", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "outputMappings": [{"definition": {"name": "result"}, "mappedOutput": {"definition": {"name": "result"}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}}], "requiredResources": [{"resourceKey": "s3"}, {"resourceKey": "spark"}, {"resourceKey": "file_cache"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "s3_to_df", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependedBy": [{"definition": {"name": "data_frame", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "subsample_spark_dataset"}}]}]}}, {"handleID": "process_q2_market_data.s3_to_df.cache_file_from_s3", "parent": {"handleID": "process_q2_market_data.s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": "Optionally specify the key for the file to be ingested into the keyed store. Defaults to the last path component of the downloaded s3 key.", "isOptional": true, "name": "file_key"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.47", "name": null}}, "description": "This is a solid which caches a file in s3 into file cache.\n\nThe `file_cache` is a resource type that allows a solid author to save files\nand assign a key to them. The keyed file store can be backed by local file or any\nobject store (currently we support s3). This keyed file store can be configured\nto be at an external location so that is persists in a well known spot between runs.\nIt is designed for the case where there is an expensive download step that should not\noccur unless the downloaded file does not exist. Redownload can be instigated either\nby configuring the source to overwrite files or to just delete the file in the underlying\nstorage manually.\n\nThis works by downloading the file to a temporary file, and then ingesting it into\nthe file cache. In the case of a filesystem-backed file cache, this is a file\ncopy. In the case of a object-store-backed file cache, this is an upload.\n\nIn order to work this must be executed within a mode that provides an `s3`\nand `file_cache` resource.\n    ", "inputDefinitions": [{"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}], "metadata": [], "name": "cache_file_from_s3", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}], "requiredResources": [{"resourceKey": "s3"}, {"resourceKey": "file_cache"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}], "name": "cache_file_from_s3", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "archive_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}]}}, {"handleID": "process_q2_market_data.s3_to_df.ingest_csv_file_handle_to_spark", "parent": {"handleID": "process_q2_market_data.s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Take a file handle that contains a csv with headers and load it\ninto a Spark DataFrame. It infers header names but does *not* infer schema.\n\nIt also ensures that the column names are valid parquet column names by\nfiltering out any of the following characters from column names:\n\nCharacters (within quotations): \"`[ ,;{}()\\n\\t=]`\"\n\n", "inputDefinitions": [{"description": null, "name": "csv_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}], "metadata": [], "name": "ingest_csv_file_handle_to_spark", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "requiredResources": [{"resourceKey": "spark"}]}, "inputs": [{"definition": {"description": null, "name": "csv_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}], "name": "ingest_csv_file_handle_to_spark", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependedBy": []}]}}, {"handleID": "process_q2_market_data.s3_to_df.unzip_file_handle", "parent": {"handleID": "process_q2_market_data.s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Unzip a file that is resident in an archive file as a member.\n    This solid operates on FileHandles, meaning that their physical is dependent\n    on what system storage is operating in the pipeline. The physical file could\n    be on local disk, or it could be in s3. If on s3, this solid will download\n    that file to local disk, perform the unzip, upload that file back to s3, and\n    then return that file handle for downstream use in the computations.\n    ", "inputDefinitions": [{"description": null, "name": "archive_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}], "metadata": [], "name": "unzip_file_handle", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}], "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "archive_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "cache_file_from_s3"}}]}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "unzip_file_handle", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "csv_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}]}]}}, {"handleID": "process_q2_market_data.subsample_spark_dataset", "parent": {"handleID": "process_q2_market_data"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "Int"}, "description": "The integer percentage of rows to sample from the input dataset.", "isOptional": false, "name": "subsample_pct"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "Int", "name": "Int"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.166", "name": null}}, "description": "Subsample a spark dataset via the configuration option.", "inputDefinitions": [{"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "metadata": [], "name": "subsample_spark_dataset", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "s3_to_df"}}]}], "name": "subsample_spark_dataset", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependedBy": [{"definition": {"name": "data_frame", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "canonicalize_column_names"}}]}]}}, {"handleID": "process_q2_ticket_data", "parent": null, "solid": {"definition": {"__typename": "CompositeSolidDefinition", "description": "Ingest zipped csv file from s3, load into a Spark\nDataFrame, optionally subsample it (via configuring the\nsubsample_spark_dataset, solid), canonicalize the column names, and then\nload it into a data warehouse.\n", "inputDefinitions": [{"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}], "inputMappings": [{"definition": {"name": "s3_coordinate"}, "mappedInput": {"definition": {"name": "s3_coordinate"}, "solid": {"name": "s3_to_df"}}}, {"definition": {"name": "archive_member"}, "mappedInput": {"definition": {"name": "archive_member"}, "solid": {"name": "s3_to_df"}}}], "metadata": [], "name": "s3_to_dw_table", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": null, "displayName": "String", "name": "String"}}], "outputMappings": [{"definition": {"name": "result"}, "mappedOutput": {"definition": {"name": "table_name"}, "solid": {"name": "load_data_to_database_from_spark"}}}], "requiredResources": [{"resourceKey": "s3"}, {"resourceKey": "spark"}, {"resourceKey": "file_cache"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "process_q2_ticket_data", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependedBy": []}]}}, {"handleID": "process_q2_ticket_data.canonicalize_column_names", "parent": {"handleID": "process_q2_ticket_data"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": null, "inputDefinitions": [{"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "metadata": [], "name": "canonicalize_column_names", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "subsample_spark_dataset"}}]}], "name": "canonicalize_column_names", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependedBy": [{"definition": {"name": "data_frame", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "load_data_to_database_from_spark"}}]}]}}, {"handleID": "process_q2_ticket_data.load_data_to_database_from_spark", "parent": {"handleID": "process_q2_ticket_data"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": "", "isOptional": false, "name": "table_name"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.165", "name": null}}, "description": null, "inputDefinitions": [{"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "metadata": [], "name": "load_data_to_database_from_spark", "outputDefinitions": [{"description": null, "name": "table_name", "type": {"description": null, "displayName": "String", "name": "String"}}], "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "canonicalize_column_names"}}]}], "name": "load_data_to_database_from_spark", "outputs": [{"definition": {"description": null, "name": "table_name", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependedBy": []}]}}, {"handleID": "process_q2_ticket_data.s3_to_df", "parent": {"handleID": "process_q2_ticket_data"}, "solid": {"definition": {"__typename": "CompositeSolidDefinition", "description": "Ingest a zipped csv file from s3,\nstash in a keyed file store (does not download if already\npresent by default), unzip that file, and load it into a\nSpark Dataframe. See documentation in constituent solids for\nmore detail.", "inputDefinitions": [{"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}], "inputMappings": [{"definition": {"name": "s3_coordinate"}, "mappedInput": {"definition": {"name": "s3_coordinate"}, "solid": {"name": "cache_file_from_s3"}}}, {"definition": {"name": "archive_member"}, "mappedInput": {"definition": {"name": "archive_member"}, "solid": {"name": "unzip_file_handle"}}}], "metadata": [], "name": "s3_to_df", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "outputMappings": [{"definition": {"name": "result"}, "mappedOutput": {"definition": {"name": "result"}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}}], "requiredResources": [{"resourceKey": "s3"}, {"resourceKey": "spark"}, {"resourceKey": "file_cache"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "s3_to_df", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependedBy": [{"definition": {"name": "data_frame", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "subsample_spark_dataset"}}]}]}}, {"handleID": "process_q2_ticket_data.s3_to_df.cache_file_from_s3", "parent": {"handleID": "process_q2_ticket_data.s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": "Optionally specify the key for the file to be ingested into the keyed store. Defaults to the last path component of the downloaded s3 key.", "isOptional": true, "name": "file_key"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.47", "name": null}}, "description": "This is a solid which caches a file in s3 into file cache.\n\nThe `file_cache` is a resource type that allows a solid author to save files\nand assign a key to them. The keyed file store can be backed by local file or any\nobject store (currently we support s3). This keyed file store can be configured\nto be at an external location so that is persists in a well known spot between runs.\nIt is designed for the case where there is an expensive download step that should not\noccur unless the downloaded file does not exist. Redownload can be instigated either\nby configuring the source to overwrite files or to just delete the file in the underlying\nstorage manually.\n\nThis works by downloading the file to a temporary file, and then ingesting it into\nthe file cache. In the case of a filesystem-backed file cache, this is a file\ncopy. In the case of a object-store-backed file cache, this is an upload.\n\nIn order to work this must be executed within a mode that provides an `s3`\nand `file_cache` resource.\n    ", "inputDefinitions": [{"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}], "metadata": [], "name": "cache_file_from_s3", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}], "requiredResources": [{"resourceKey": "s3"}, {"resourceKey": "file_cache"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}], "name": "cache_file_from_s3", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "archive_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}]}}, {"handleID": "process_q2_ticket_data.s3_to_df.ingest_csv_file_handle_to_spark", "parent": {"handleID": "process_q2_ticket_data.s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Take a file handle that contains a csv with headers and load it\ninto a Spark DataFrame. It infers header names but does *not* infer schema.\n\nIt also ensures that the column names are valid parquet column names by\nfiltering out any of the following characters from column names:\n\nCharacters (within quotations): \"`[ ,;{}()\\n\\t=]`\"\n\n", "inputDefinitions": [{"description": null, "name": "csv_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}], "metadata": [], "name": "ingest_csv_file_handle_to_spark", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "requiredResources": [{"resourceKey": "spark"}]}, "inputs": [{"definition": {"description": null, "name": "csv_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}], "name": "ingest_csv_file_handle_to_spark", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependedBy": []}]}}, {"handleID": "process_q2_ticket_data.s3_to_df.unzip_file_handle", "parent": {"handleID": "process_q2_ticket_data.s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Unzip a file that is resident in an archive file as a member.\n    This solid operates on FileHandles, meaning that their physical is dependent\n    on what system storage is operating in the pipeline. The physical file could\n    be on local disk, or it could be in s3. If on s3, this solid will download\n    that file to local disk, perform the unzip, upload that file back to s3, and\n    then return that file handle for downstream use in the computations.\n    ", "inputDefinitions": [{"description": null, "name": "archive_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}], "metadata": [], "name": "unzip_file_handle", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}], "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "archive_file_handle", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "cache_file_from_s3"}}]}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "unzip_file_handle", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": null, "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "csv_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}]}]}}, {"handleID": "process_q2_ticket_data.subsample_spark_dataset", "parent": {"handleID": "process_q2_ticket_data"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "Int"}, "description": "The integer percentage of rows to sample from the input dataset.", "isOptional": false, "name": "subsample_pct"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "Int", "name": "Int"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.166", "name": null}}, "description": "Subsample a spark dataset via the configuration option.", "inputDefinitions": [{"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "metadata": [], "name": "subsample_spark_dataset", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "s3_to_df"}}]}], "name": "subsample_spark_dataset", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependedBy": [{"definition": {"name": "data_frame", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "canonicalize_column_names"}}]}]}}, {"handleID": "process_sfo_weather_data", "parent": null, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": null, "inputDefinitions": [{"description": null, "name": "sfo_weather_data", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "metadata": [], "name": "process_sfo_weather_data", "outputDefinitions": [{"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}], "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "sfo_weather_data", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "ingest_q2_sfo_weather"}}]}], "name": "process_sfo_weather_data", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "PySparkDataFrame", "name": "PySparkDataFrame"}}, "dependedBy": [{"definition": {"name": "data_frame", "type": {"displayName": "PySparkDataFrame"}}, "solid": {"name": "load_q2_sfo_weather"}}]}]}}], "solids": [{"name": "april_on_time_s3_to_df"}, {"name": "download_q2_sfo_weather"}, {"name": "ingest_q2_sfo_weather"}, {"name": "join_q2_data"}, {"name": "june_on_time_s3_to_df"}, {"name": "load_q2_on_time_data"}, {"name": "load_q2_sfo_weather"}, {"name": "master_cord_s3_to_df"}, {"name": "may_on_time_s3_to_df"}, {"name": "process_q2_coupon_data"}, {"name": "process_q2_market_data"}, {"name": "process_q2_ticket_data"}, {"name": "process_sfo_weather_data"}]}}}
