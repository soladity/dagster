import PyObject from 'components/PyObject';
import { DynamicMetaTags } from 'components/MetaTags';

<DynamicMetaTags
  title="The Dagster instance | Dagster"
  description="Dagster Instance defines everything Dagster needs for a single deployment."
/>

# The Dagster instance

The <PyObject module="dagster" object="DagsterInstance" displayText="DagsterInstance" />
defines all of the configuration that Dagster needs for a single deployment - for example,
where to store the history of past runs and their associated logs, where to stream the raw logs
from solid compute functions, how to store local artifacts on disk, and how to launch new runs.

All of the processes and services that make up your Dagster deployment should share a single instance
config file so that they can effectively share information.

**NOTE:** Some important configuration, like execution parallelism, is also set on a
per-pipeline-run basis rather than on the instance - see the section below on pipeline configuration.

## Default local behavior

When you launch a Dagster process, like Dagit or the Dagster CLI commands, Dagster attempts to load
your instance. If the environment variable `DAGSTER_HOME` is set, Dagster will look for an
instance config file at `$DAGSTER_HOME/dagster.yaml`. This file contains each of the configuration
settings that make up the instance.

By default (if `dagster.yaml` is not present or nothing is specified in that file), Dagster will
store this information on the local filesystem, laid out like this:

```tree
$DAGSTER_HOME
├── dagster.yaml
├── history
│   ├── runs
│   │   ├── 00636713-98a9-461c-a9ac-d049407059cd.db
│   │   └── ...
│   └── runs.db
└── storage
    ├── 00636713-98a9-461c-a9ac-d049407059cd
    │   ├── compute_logs
    │   │   ├── my_solid.compute.complete
    │   │   ├── my_solid.compute.err
    │   │   ├── my_solid.compute.out
    │   │   └── ...
    │   └── intermediates
    │       ├── my_solid.compute
    │       │   ├── output_one
    │       │   └── ...
    │       └── ...
    └── ...
```

The `runs.db` and `{run_id}.db` files are SQLite database files recording information about pipeline
runs and per-run event logs respectively. The `compute_logs` directories (one per
pipeline run) contain the stdout and stderr logs from the execution of the compute functions of each
solid in a pipeline. Finally, the `intermediates` directories contain serialized representations of the
outputs of each solid, enabling incremental recomputation of pipeline subsets.

If `DAGSTER_HOME` is not set, the Dagster tools will use an ephemeral instance for execution. In
this case, the run and event log storages will be in-memory rather than persisted to disk, and
filesystem storage will use a temporary directory that is cleaned up when the process exits. This is
useful for tests and is the default when invoking Python APIs such as <PyObject module="dagster"
object="execute_pipeline" displayText="execute_pipeline" /> directly.

## Instance Configuration YAML

In persistent Dagster deployments, you will typically want to configure many of the components on
the instance. For example, you may want to use a Postgres instance to store runs and the corresponding event logs,
to store intermediates and other local artifacts on an NFS mount, and to stream compute logs to an
S3 bucket.

To do this, provide a `$DAGSTER_HOME/dagster.yaml` file. Dagit and all Dagster tools will look for
this file on startup. Note that Dagster supports retrieving instance YAML values from environment
variables, using `env:` instead of a string literal. An example `dagster.yaml` is below:

```yaml literalinclude caption=dagster.yaml
file:/docs_snippets/docs_snippets/overview/instances/dagster.yaml
```

The API docs for available options are linked below:

**Run Storage**

- _(default)_ <PyObject module="dagster.core.storage.runs" object="SqliteRunStorage" />
- <PyObject module="dagster_postgres" object="PostgresRunStorage" />

**Event Log Storage**

- _(default)_ <PyObject module="dagster.core.storage.event_log" object="SqliteEventLogStorage" />
- <PyObject module="dagster_postgres" object="PostgresEventLogStorage" />

**Compute Log Manager**

- _(default)_ <PyObject module="dagster.core.storage.local_compute_log_manager" object="LocalComputeLogManager" />
- <PyObject module="dagster_aws.s3" object="S3ComputeLogManager" />

**Local Artifact Storage**

- _(default)_ <PyObject module="dagster.core.storage.root" object="LocalArtifactStorage" />

**Scheduler**

- _(default)_ <PyObject module="dagster.core.scheduler" object="DagsterDaemonScheduler" />
- _(deprecated)_ <PyObject module="dagster_cron.cron_scheduler" object="SystemCronScheduler" />

**Scheduler Storage**

- _(default)_ <PyObject module="dagster.core.storage.schedules" object="SqliteScheduleStorage"/>
- <PyObject module="dagster_postgres" object="PostgresScheduleStorage" />

**Run Launcher**

- _(default)_ <PyObject module="dagster.core.launcher" object="DefaultRunLauncher" />

**Run Coordinator**

- _(default)_ <PyObject module="dagster.core.run_coordinator" object="DefaultRunCoordinator" />
- <PyObject
    module="dagster.core.run_coordinator"
    object="QueuedRunCoordinator"
  />

## Pipeline Run Configuration YAML

Along with instance-level configuration, Dagster supports configuring several properties on a
per-pipeline-run basis. An example is shown below:

```yaml literalinclude caption=pipeline_run.yaml
file:/docs_snippets/docs_snippets/overview/instances/pipeline_run.yaml
```
