import AnchorHeading from 'components/AnchorHeading';
import { DynamicMetaTags } from 'components/MetaTags';

<DynamicMetaTags
  title="Pandas | Dagster"
  description="The dagster-pandas library provides the ability to perform data validation, emit
  summary statistics, and enable reliable dataframe serialization/deserialization."
/>

# Validating Pandas DataFrames with Dagster Types

- [Creating Dagster DataFrame Types](/overview/packages/dagster_pandas#creating-dagster-dataframe-types)
- [Dagster DataFrame Level Validation](/overview/packages/dagster_pandas#dagster-dataframe-level-validation)
- [Dagster DataFrame Custom Validation](/overview/packages/dagster_pandas#dagster-dataframe-custom-validation)

The `dagster_pandas` library provides the ability to perform data validation, emit summary
statistics, and enable reliable dataframe serialization/deserialization. On top of this, the Dagster
type system generates documentation of your dataframe constraints and makes it accessible in the
Dagit UI.

## Creating Dagster DataFrame Types

To create a custom `dagster_pandas` type, use `create_dagster_pandas_dataframe_type` and provide a
list of `PandasColumn` objects which specify column-level schema and constraints. For example, we
can construct a custom dataframe type to represent a set of e-bike trips in the following way:

```python literalinclude caption=core_trip_pipeline.py
file:/docs_snippets/docs_snippets/legacy/dagster_pandas_guide/core_trip_pipeline.py
startAfter:start_core_trip_pipeline_marker_0
endBefore:end_core_trip_pipeline_marker_0
```

Once our custom data type is defined, we can use it as the type declaration for the inputs / outputs
of our solid:

```python literalinclude caption=hello_cereal.py
file:/docs_snippets/docs_snippets/legacy/dagster_pandas_guide/core_trip_pipeline.py
startAfter:start_core_trip_pipeline_marker_1
endBefore:end_core_trip_pipeline_marker_1
```

By passing in these `PandasColumn` objects, we are expressing the schema and constraints we expect
our dataframes to follow when Dagster performs type checks for our solids. Moreover, if we go to the
solid viewer, we can follow our schema documented in Dagit:

![tutorial2.png](/assets/images/learn/guides/dagster_pandas/tutorial2.png)

## Dagster DataFrame Level Validation

Now that we have a custom dataframe type that performs schema validation during a pipeline run, we
can express dataframe level constraints (e.g number of rows, or columns).

To do this, we provide a list of dataframe constraints to `create_dagster_pandas_dataframe_type`;
for example, using `RowCountConstraint`. More information on the available constraints can be found
in the `dagster_pandas` [API docs](/_apidocs/libraries/dagster_pandas).

This looks like:

```python literalinclude caption=shape_constrained_pipeline.py
file:/docs_snippets/docs_snippets/legacy/dagster_pandas_guide/shape_constrained_pipeline.py
lines:9-11
```

If we rerun the above example with this dataframe, nothing should
change. However, if we pass in 100 to the row count constraint, we can
watch our pipeline fail that type check.

## Dagster DataFrame Summary Statistics

Aside from constraint validation, `create_dagster_pandas_dataframe_type`
also takes in a summary statistics function that emits
`EventMetadataEntry` objects which are surfaced during pipeline runs.
Since data systems seldom control the quality of the data they receive,
it becomes important to monitor data as it flows through your systems.
In complex pipelines, this can help debug and monitor data drift over
time. Let's illustrate how this works in our example:

```python literalinclude caption=summary_stats_pipeline.py
file:/docs_snippets/docs_snippets/legacy/dagster_pandas_guide/summary_stats_pipeline.py
lines:10-39
```

Now if we run this pipeline in the Dagit playground:

![tutorial1.png](/assets/images/learn/guides/dagster_pandas/tutorial1.png)

# Dagster DataFrame Custom Validation

`PandasColumn` is user-pluggable with custom constraints. They can be constructed directly and
passed a list of `ColumnConstraint` objects.

To tie this back to our example, let's say that we want to validate that the amount paid for a
e-bike must be in 5 dollar increments because that is the price per mile rounded up. As a result,
let's implement a `DivisibleByFiveConstraint`. To do this, all it needs is a `markdown_description`
for Dagit which accepts and renders markdown syntax, an `error_description` for error logs, and a
validation method which throws a `ColumnConstraintViolationException` if a row fails validation.
This would look like the following:

```python literalinclude emphasize-lines=22-25 caption=custom_column_constraint_pipeline.py
file:/docs_snippets/docs_snippets/legacy/dagster_pandas_guide/custom_column_constraint_pipeline.py
lines:15-40
```
