# Lakehouse with Pandas and Pyspark

Lakehouse is an experimental API built on top of Dagster's core abstractions that makes it easy to
define computations in terms of the data assets that they produce. In the last example, we
demonstrated how to use Lakehouse to transform data assets into a core dagster pipeline. In this
example, we'll be demonstrating how we can use Lakehouse to construct a dagster pipeline with
multiple compute options. Users unfamiliar with lakehouse should first view the first lakehouse
tutorial [here](https://docs.dagster.io/examples/simple_lakehouse).

Different computable assets will often ingest data in different ways. Let's say we have
 a table of temperature samples collected in 5 minute increments, and we want to compute the high
 temperature for each day
represented in the table. In addition, we want to compute the difference between the high
temperature of consecutive days. This pipeline will ingest a csv file as a pandas dataframe, and
outputted the computed high temperatures again as a csv file. Then, we want to ingest the computed
high temperatures as a Spark Dataframe, and then output the differences back to csv files.
First, we would need to utilize a storage mechanism that is digestible by Spark. In this case,
that means representing tables as _directories_ of csv files as opposed to a single csv file.
From these directories of csv files, we must be able to compute both pandas dataframes and spark
dataframes. How do we utilize both the pandas conversion and the spark conversion?

Luckily, Lakehouse provides a way to _compose_ different types of storage. To see how,
let's first define our storage for converting between a folder of csv files and pandas dataframe.

In the below code examples, note that PandasDF refers to the pandas dataframe class,
`import pandas.Dataframe as PandasDF`, and SparkDF refers to the spark dataframe class,
`import spark.Dataframe as SparkDF`.

## Data Assets

We'll use Assets to define each of the tables.

```python literalinclude caption=assets.py
file:/multi_type_lakehouse/multi_type_lakehouse/assets.py
```

`sfo_q2_weather_sample_table` represents our base temperature table. Passing in `"filesystem"` for
the `storage_key` argument indicates that this asset is stored locally. The `path` argument gives
the path to the data asset itself.

`daily_temperature_highs_table` represents our computed high temperatures. We explicitly define the
dependency on the original table by passing `sfo_q2_weather_sample_table` as the value for the
`input_deps` argument.

We have an additional table `daily_temperature_high_diffs_table` that
represents the difference between the high temperature of consecutive days. We use the
`input_assets` parameter to make explicit the dependency on `daily_temperature_highs_table`.

## Storage

The ingestion of csv files is a bit different this time, because our specification requires
ingesting a folder of csv files as opposed to a single csv file.

```python literalinclude caption=lakehouse_def.py
file:/multi_type_lakehouse/multi_type_lakehouse/lakehouse_def.py
lines:23-72
```

The `load` method takes in all csv files within a given directory, rather than specifying
a csv file or set of csv files explicitly. Analogously, the `save` method writes csv files to a
directory in parts.

We'll do something similar for conversion between a csv and a spark datafarame:
```python literalinclude caption=lakehouse_def.py
file:/multi_type_lakehouse/multi_type_lakehouse/lakehouse_def.py
lines:75-92
```

To compose these two compute types, we utilize the `multi_type_asset_storage` function
provided by lakehouse:
```python
from lakehouse import multi_type_asset_storage
```

We can now define a composed AssetStorage:

```python literalinclude caption=lakehouse_def.py
file:/multi_type_lakehouse/multi_type_lakehouse/lakehouse_def.py
lines:95-98
```

Finally, we construct our lakehouse:

```python literalinclude caption=lakehouse_def.py
file:/multi_type_lakehouse/multi_type_lakehouse/lakehouse_def.py
lines:101-110
```

## Pipeline

Once again, using the data assets and storage for handling conversion, we have completely defined
our computation graph.
To construct a pipeline from these assets:
```python literalinclude caption=pipelines.py
file:/multi_type_lakehouse/multi_type_lakehouse/pipelines.py
```

Note that the assets don't have to be provided in order. Lakehouse is able to determine asset
ordering by resolving input asset dependencies.




## Open in a playground

[![Open in Gitpod](https://gitpod.io/button/open-in-gitpod.svg)](https://gitpod.io/#EXAMPLE=multi_type_lakehouse/https://github.com/dagster-io/dagster)

## Download

```
curl https://codeload.github.com/dagster-io/dagster/tar.gz/master | tar -xz --strip=2 dagster-master/examples/multi_type_lakehouse
cd multi_type_lakehouse
pip install -e .
```
