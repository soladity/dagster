import { DynamicMetaTags } from 'components/MetaTags';

<DynamicMetaTags
  title="Making Your Pipelines Testable and Maintainable | Dagster"
  description="Dagster is a system for building modern data applications."
/>

import AnchorHeading from 'components/AnchorHeading';
import PyObject from 'components/PyObject';

## Making Your Pipelines Testable and Maintainable

The reality of building data applications is that they are notoriously difficult to test and are
therefore typically un- or under-tested. Besides, pipeline authors generally do not have control
over their input data, which led to an even more unfortunate reality — even if data pipelines are
covered by sophisticated tests, pipeline breakage could still happen.

Creating testable and verifiable data applications is one of the focuses of Dagster. We believe
ensuring data quality is critical for managing the complexity of data systems. This section will
talk about how you can build maintainable and testable data applications with Dagster.

> - You can find the tutorial code on [Github](https://github.com/dagster-io/dagster/tree/master/examples/dagster_examples/intro_tutorial/basics/e04_quality/)
> - If you’ve cloned the dagster git repository, you’ll find this example at `examples/dagster_examples/intro_tutorial/basics/e04_quality/`

### Testing solids and pipelines

Let's go back to our first solid and pipeline [`hello_cereal.py`](#execute-our-first-pipeline).

It wouldn't be complete without some tests to ensure they're working as expected. We'll use <PyObject module="dagster" object="execute_pipeline" displayText="execute_pipeline()" />
to test our pipeline, as well as <PyObject module="dagster" object="execute_solid" displayText="execute_solid()" />
to test our solid in isolation.

These functions synchronously execute a pipeline or solid and return results
objects (the <PyObject module="dagster" object="SolidExecutionResult" /> and <PyObject module="dagster" object="PipelineExecutionResult" />)
whose methods let us investigate, in detail, the success or failure of execution,
the outputs produced by solids, and (as we'll see later) other events associated
with execution.

```python literalinclude showLines startLine=32 caption=hello_cereal.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/hello_cereal_with_tests.py
lines:32-43
```

Now you can use pytest, or your test runner of choice, to run unit tests as
you develop your data applications.

```bash
$ pytest hello_cereal_with_tests.py
```

Note: by convention, pytest tests are typically kept in separate files
prefixed with `test_`. We've put them in the same file just to
simplify the tutorial code.

Obviously, in production we'll often execute pipelines in a parallel,
streaming way that doesn't admit this kind of API, which is intended to enable
local tests like this.

Dagster is written to make testing easy in a domain where it has historically been very difficult. Throughout the rest of this tutorial, we'll explore the writing of unit tests for each piece of the framework as we learn about it. You can learn more about Testing in Dagster by reading [Testing Guide](/docs/learn/guides/testing/testing).

<br />

### Type-checking Inputs with Python Type Annotation

Note that this section requires Python 3.

If you zoom in on the **Explore** tab in Dagit and click on one
of our pipeline solids, you'll see that its inputs and outputs are annotated
with types.

![inputs_figure_four.png](/assets/images/tutorial/inputs_figure_four.png)

By default, every untyped value in Dagster is assigned the catch-all type <PyObject module="dagster" object="Any" />.
This means that any errors in the config won't be surfaced until the pipeline
is executed.

For example, when we execute our pipeline with this config, it'll fail at
runtime:

```YAML literalinclude caption=inputs_env_bad.yaml
file:/dagster_examples/intro_tutorial/basics/e04_quality/inputs_env_bad.yaml
```

When we enter this mistyped config in Dagit and execute our pipeline, you'll
see that an error appears in the structured log viewer pane of the **Execute**
tab:

![inputs_figure_five.png](/assets/images/tutorial/inputs_figure_five.png)

Click on "View Full Message" or on the red dot on the execution step that
failed and a detailed stacktrace will pop up.

![inputs_figure_six.png](/assets/images/tutorial/inputs_figure_six.png)

It would be better if we could catch this error earlier, when we specify the
config. So let's make the inputs typed.

A user can apply types to inputs and outputs using Python 3's type annotation
syntax. In this case, we just want to type the input as the built-in `str`.

```python literalinclude showLines startLine=7 emphasize-lines=2 caption=inputs_typed.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/inputs_typed.py
lines:7-15
```

By using typed input instead we can catch this error prior to execution, and
reduce the surface area we need to test and guard against in user code.

![inputs_figure_seven.png](/assets/images/tutorial/inputs_figure_seven.png)

<br />

### Dagster Types

We've seen how we can type the inputs and outputs of solids using
Python 3's typing system, and how to use Dagster's built-in config
types, such as <PyObject module="dagster" object="String" displayText="dagster.String" />, to
define config schemas for our solids.

But what about when you want to define your own types?

This section will talk about the basics of Dagster's user defined types in the following order. You
can also learn more about Typing in Dagster by reading [Dagster Types Guide](/docs/learn/guides/dagster_types).

- [Complex Type Checks](#complex-type-checks)
- [Providing Input Values for Custom Types in Config](#providing-input-values-for-custom-types-in-config)
- [Testing Custom Types](#testing-custom-types)
- [MyPy Compliance](#mypy-compliance)
- [Metadata and Custom Type Checks](#Metadata-and-custom-type-checks)

<br />

Let's look back at our simple `read_csv` solid.

```python literalinclude showLines startLine=7 caption=inputs_typed.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/inputs_typed.py
lines:7-15
```

The `lines` object returned by Python's built-in `csv.DictReader` is a
list of `collections.OrderedDict`, each of which represents one row of
the dataset:

```python
[
    OrderedDict([
        ('name', '100% Bran'), ('mfr', 'N'), ('type', 'C'), ('calories', '70'), ('protein', '4'),
        ('fat', '1'), ('sodium', '130'), ('carbo', '5'), ('sugars', '6'), ('potass', '280'),
        ('vitamins', '25'), ('shelf', '3'), ('weight', '1'), ('cups', '0.33'),
        ('rating', '68.402973')
    ]),
    OrderedDict([
        ('name', '100% Natural Bran'), ('mfr', 'Q'), ('type', 'C'), ('calories', '120'),
        ('protein', '3'), ('fat', '5'), ('sodium', '15'), ('fiber', '2'), ('carbo', '8'),
        ('sugars', '8'), ('potass', '135'), ('vitamins', '0'), ('shelf', '3'), ('weight', '1'),
        ('cups', '1'), ('rating', '33.983679')
    ]),
    ...
]
```

This is a simple representation of a "data frame", or a table of data.
We'd like to be able to use Dagster's type system to type the output
of `read_csv`, so that we can do type checking when we construct the
pipeline, ensuring that any solid consuming the output of `read_csv`
expects to receive a data frame.

To do this, we'll use the <PyObject module="dagster" object="DagsterType" displayText="DagsterType" /> class:

```python literalinclude  showLines startLine=6 caption=custom_types.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_types.py
lines:6-10
```

Now we can annotate the rest of our pipeline with our new type:

```python literalinclude showLines startLine=13 emphasize-lines=2,12 caption=custom_types.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_types.py
lines:13-36
```

The type metadata now appears in dagit and the system will ensure the
input and output to this solid are indeed instances of SimpleDataFrame.
As usual, run:

```bash
$ dagit -f custom_types.py -n custom_type_pipeline
```

![custom_types_figure_one.png](/assets/images/tutorial/custom_types_figure_one.png)

You can see that the output of `read_csv` (which by default has the name
`result`) is marked to be of type `SimpleDataFrame`.

<br />

#### Complex Type Checks

The Dagster framework will fail type checks when a value isn't an
instance of the type we're expecting, e.g., if `read_csv` were to
return a `str` rather than a `SimpleDataFrame`.

Sometimes we know more about the types of our values, and we'd like to
do deeper type checks. For example, in the case of the
`SimpleDataFrame`, we expect to see a list of OrderedDicts, and for each
of these OrderedDicts to have the same fields, in the same order.

The type check function allows us to do this.

```python literalinclude showLines startLine=7 emphasize-lines=1,21 caption=custom_types_2.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_types_2.py
lines:7-30
```

Now, if our solid logic fails to return the right type, we'll see a
type check failure. Let's replace our `read_csv` solid with the
following bad logic:

```python literalinclude showLines startLine=32 caption=custom_types_2.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_types_2.py
lines:32-39
```

When we run the pipeline with this solid, we'll see an error like:

```bash
2019-10-12 13:19:19 - dagster - ERROR - custom_type_pipeline - 266c6a93-75e2-46dc-8bd7-d684ce91d0d1 - STEP_FAILURE
- Execution of step "bad_read_csv.compute" failed.
            cls_name = "Failure"
            solid = "bad_read_csv"
    solid_definition = "bad_read_csv"
            step_key = "bad_read_csv.compute"
user_failure_data = {"description": "LessSimpleDataFrame should be a list of OrderedDicts, got <class 'str'>
for row 1", "label": "intentional-failure", "metadata_entries": []}
```

<br />

#### Providing Input Values for Custom Types in Config

We saw earlier how, when a solid doesn't receive all of its inputs from
other solids further upstream in the pipeline, we can specify its input
values in config:

```YAML literalinclude caption=inputs_env.yaml
file:/dagster_examples/intro_tutorial/basics/e02_solids/inputs_env.yaml
```

The Dagster framework knows how to interpret values provided via config
as scalar inputs. In this case, `read_csv` just takes the string
representation of the filepath from which it'll read a csv. But for
more complex, custom types, we need to tell Dagster how to interpret
config values.

Consider our LessSimpleDataFrame. It might be convenient if Dagster knew
automatically how to read a data frame in from a csv file, without us
needing to separate that logic into the `read_csv` solid -- especially
if we knew the provenance and format of that csv file (e.g., if we were
using standard csvs as an internal interchange format) and didn't need
the full configuration surface of a general purpose `read_csv` solid.

What we want to be able to do is write:

```YAML literalinclude caption=custom_type_input.yaml
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_type_input.yaml
```

In order for the Dagster machinery to be able to decode the config value
`{'csv': 'cereal.csv'}` into an input of the correct
`LessSimpleDataFrame` value, we need to write what we call an input
hydration config.

```python literalinclude showLines startLine=32 caption=custom_types_3.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_types_3.py
lines:32-39
```

A function decorated with <PyObject module="dagster" object="input_hydration_config" displayText="@input_hydration_config" /> should
take the context object, as usual, and a parameter
representing the parsed config field. The schema for this field is
defined by the argument to the <PyObject module="dagster" object="input_hydration_config" displayText="@input_hydration_config" /> decorator.

Here, we introduce the <PyObject module="dagster" object="Selector" /> type,
which lets you specify mutually exclusive options in config schemas.
Here, there's only one option, `csv`, but you can imagine a more
sophisticated data frame type that might also know how to hydrate its
inputs from other formats and sources, and might have a selector with
fields like `parquet`, `xlsx`, `sql`, etc.

Then insert this into the original declaration:

```python literalinclude showLines startLine=42 emphasize-lines=5 caption=custom_types_3.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_types_3.py
lines:42-47
```

Now if you run a pipeline with this solid from dagit you will be able to
provide sources for these inputs via config:

```python literalinclude showLines startLine=70 caption=custom_types_3.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_types_3.py
lines:70-80
```

<br />

#### Testing Custom Types

As you write your own custom types, you'll also want to set up unit
tests that ensure your types are doing what you expect them to. Dagster
includes a utility function, <PyObject module="dagster" object="check_dagster_type" />,
that lets you type check any Dagster type against any value.

```python literalinclude showLines startLine=102 caption=custom_types_test.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_types_test.py
lines:102-114
```

Well tested library types can be reused across solids and pipelines to
provide standardized type checking within your organization's data
applications.

<br />

#### MyPy Compliance

In cases where DagsterTypes are created that do not have corresponding
usable Python types, and the user wishes to remain mypy compliant, there
are two options.

One is using `InputDefinition` and `OutputDefinition` exclusively for
dagster types, and reserving type annotations for naked Python types
_only_. This is verbose, but is explicit and clear.

```python literalinclude showLines startLine=21 emphasize-lines=1-5,14-15 caption=custom_types_mypy_verbose.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_types_mypy_verbose.py
lines:21-47
```

If one wishes to use type annotations exclusively but still use dagster
types without a 1:1 python type counterpart, the typechecking behavior
must be modified. For this we recommend using the `typing.TYPE_CHECKING`
property in the python typing module.

While inelegant, this centralizes boilerplate to the type instantiation,
rather than have it on all places where the type is referenced.

```python literalinclude showLines startLine=7 emphasize-lines=1-8 caption=custom_types_mypy_typing_trick.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_types_mypy_typing_trick.py
lines:7-25
```

<br />

#### Metadata and Custom Type Checks

Custom types can also yield metadata about the type check. For example, in the
case of our data frame, we might want to record the number of rows and columns
in the dataset when our type checks succeed, and provide more information
about why type checks failed when they fail.
User-defined type check functions can optionally return a <PyObject module="dagster" object="TypeCheck" /> object
that contains metadata about the success or failure of the type check.
Let's see how to use this to emit some summary statistics about our DataFrame
type:

```python literalinclude showLines startLine=18 emphasize-lines=3-9,33-53 caption=custom_types_4.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_types_4.py
lines:18-71
```

A <PyObject module="dagster" object="TypeCheck" displayText="TypeCheck" /> must include a
`success` argument describing whether the check
passed or failed, and may include a description and/or a list of <PyObject module="dagster" object="EventMetadataEntry" /> objects.
You should use the static constructors on <PyObject module="dagster" object="EventMetadataEntry" /> to
construct these objects, which are flexible enough to support arbitrary metadata in JSON or Markdown format.

Dagit knows how to display and archive structured metadata of this kind for
future review:

![custom_types_figure_two.png](/assets/images/tutorial/custom_types_figure_two.png)

<br />

### Expectations

Custom type checks and metadata are appropriate for checking that a value will behave as we expect, and for collecting summary information about values.

But sometimes we want to make more specific, data- and business logic-dependent assertions about the semantics of values. It typically isn't appropriate to embed assertions like these into data types directly.

For one, they will usually vary substantially between instantiations — for example, we don't expect all data frames to have the same number of columns, and over-specifying data types (e.g., `SixColumnedDataFrame`) makes it difficult for generic logic to work generically (e.g., over all
data frames).

What's more, these additional, deeper semantic assertions are often non-stationary. Typically, you'll start running a pipeline with certain expectations about the data that you'll see; but over time, you'll learn more about your data (making your expectations more precise), and the process in the world that generates your data will shift (making some of your expectations invalid.)

We've already encountered the <PyObject module="dagster" object="TypeCheck" /> event, which is typically yielded by the type machinery (but can also be yielded manually from the body of a solid's compute function); <PyObject module="dagster" object="ExpectationResult" /> is another kind of structured side-channel result that a solid can yield. These extra events don't get passed to downstream solids and they aren't used to define the data dependencies of a pipeline DAG.

```python literalinclude showLines startLine=93 emphasize-lines=1-3,31 caption=custom_types_bad_5.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_types_bad_5.py
lines:93-133
```

Until now, every solid we've encountered has returned its result value, or `None`. But solids can also yield events of various types for side-channel communication about the results of their computations.

Running this pipeline yields an <PyObject module="dagster" object="ExpectationResult" /> with `success` set to `False` since we expect entries in the `calories` column to be of type `int` but they are of type `string`. We note that this precedes our incorrect result that the least caloric cereal is Corn Flakes (100 calories per serving) and the most caloric cereal Strawberry Fruit Wheats (90 calories per serving).

![custom_types_bad_data.png](/assets/images/tutorial/custom_types_bad_data.png)

To fix this, we can cast `calories` to `int` during the hydration process:

```python literalinclude showLines startLine=75 emphasize-lines=7 caption=custom_types_5.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_types_5.py
lines:75-86
```

Running this pipeline yields an <PyObject module="dagster" object="ExpectationResult" /> with
`success` set to `True` and the correct result that
the least caloric cereal is All-Bran with Extra Fiber (50 calories per
serving) and the most caloric cereal is Mueslix Crispy Blend (160
calories per serving).

This part of this system remains relatively immature, but yielding structured expectation results from your solid logic means that in future, tools like Dagit will be able to aggregate and track expectation results, as well as implement sophisticated policy engines to drive alerting and exception handling on a deep semantic basis. You can learn more about it by reading [Data Quality Tests Guide](/docs/learn/guides/testing/expectations).

<br />

Congratulations! Having reached this far, you now have a working, testable, and maintainable data pipeline. You’ve also learned the basics of Dagster, and you should now be able to build your own data applications using Dagster!

<br />

<br />
