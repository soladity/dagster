import PyObject from 'components/PyObject';
import { DynamicMetaTags } from 'components/MetaTags';

<DynamicMetaTags
  title="The Dagster instance | Dagster"
  description="Dagster is a system for building modern data applications."
/>

# The Dagster instance

The <PyObject module="dagster.core.instance" object="DagsterInstance" displayText="DagsterInstance"
/> defines everything Dagster needs for a single deployment: where to store the history of past runs
and their associated logs, where to stream the raw logs from solid compute functions, how to store
local artifacts on disk, and how to launch new runs.

Locally, an instance usually corresponds to a single Dagit process. In production, you'll want all
of your processes to share a single instance config so they can effectively share information.

**NOTE:** many important configurations like execution parallelism are set on a per-pipeline-run
basis, please check the section below on pipeline configuration.

## Default local behavior

When you use Dagster, like running `dagit` from the command line, Dagster checks whether the
environment variable `DAGSTER_HOME` is set. If it is, Dagster will look for an instance config file
at `$DAGSTER_HOME/dagster.yaml`.

This file contains configuration settings that tell Dagster where and how to store local artifacts
like intermediates, schedules, stdout and stderr logs from solid compute functions, and information
about past runs and the structured events.

By default (if `dagster.yaml` is not present or nothing is specified in that file), Dagster will
store this information on the local filesystem, laid out like this:

```tree
$DAGSTER_HOME
├── dagster.yaml
├── history
│   ├── runs
│   │   ├── 00636713-98a9-461c-a9ac-d049407059cd.db
│   │   └── ...
│   └── runs.db
├── schedules
│   ├── my_repository
│   │   ├── my_pipeline_4c9ddc62-eeab-4937-a429-fdbc8832013c.json
│   │   └── ...
│   └── ...
└── storage
    ├── 00636713-98a9-461c-a9ac-d049407059cd
    │   ├── compute_logs
    │   │   ├── my_solid.compute.complete
    │   │   ├── my_solid.compute.err
    │   │   ├── my_solid.compute.out
    │   │   └── ...
    │   └── intermediates
    │       ├── my_solid.compute
    │       │   ├── output_one
    │       │   └── ...
    │       └── ...
    └── ...
```

The `runs.db` and `{run_id}.db` files are SQLite database files recording information about pipeline
runs and per-run event logs respectively. The `{pipeline_name}_{run_id}.json` files store
information about schedules associated with pipelines. The `compute_logs` directories (one per
pipeline run) contain the stdout and stderr logs from the execution of the compute functions of each
solid in a pipeline. And the `intermediates` directories contain serialized representations of the
outputs of each solid, enabling incremental recomputation of pipeline subsets.

If `DAGSTER_HOME` is not set, the Dagster tools will use an ephemeral instance for execution. In
this case, the run and event log storages will be in-memory rather than persisted to disk, and
filesystem storage will use a temporary directory that is cleaned up when the process exits. This is
useful for tests and is the default when invoking Python APIs such as <PyObject module="dagster"
object="execute_pipeline" displayText="execute_pipeline" /> directly.

## Instance Configuration YAML

In persistent Dagster deployments, you will typically want to configure some or all of this
behavior. When you configure an instance, you can select the the implementation of its constituent
components.

For example, you may want to use a Postgres instance to store runs and the corresponding event logs,
to store intermediates and other local artifacts on an NFS mount, and to stream compute logs to an
S3 bucket.

To do this, provide a `$DAGSTER_HOME/dagster.yaml` file. Dagit and all Dagster tools will look for
this file on startup. An example `dagster.yaml` is below:

```yaml literalinclude caption=dagster.yaml
file:src/pages/docs/deploying/dagster.yaml
relativeToProject
```

The API docs for available options are linked below:

**Run Storage**

- _(default)_ <PyObject module="dagster.core.storage.runs" object="SqliteRunStorage" />
- <PyObject module="dagster_postgres" object="PostgresRunStorage" />

**Event Log Storage**

- _(default)_ <PyObject module="dagster.core.storage.event_log" object="SqliteEventLogStorage" />
- <PyObject module="dagster_postgres" object="PostgresEventLogStorage" />

**Compute Log Manager**

- _(default)_ <PyObject module="dagster.core.storage.local_compute_log_manager" object="LocalComputeLogManager" />
- <PyObject module="dagster_aws.s3" object="S3ComputeLogManager" />

**Local Artifact Storage**

- _(default)_ <PyObject module="dagster.core.storage.root" object="LocalArtifactStorage" />

**Scheduler** _(disabled by default)_

- <PyObject module="dagster_cron.cron_scheduler" object="SystemCronScheduler" />

**Scheduler Storage**

- _(default)_ <PyObject module="dagster.core.storage.schedules" object="SqliteScheduleStorage"/>
- <PyObject module="dagster_postgres" object="PostgresScheduleStorage" />

**Run Launcher** _(none provided by default)_

- <PyObject module="dagster_graphql.launcher" object="RemoteDagitRunLauncher" />

Each of these components makes use of common plugin machinery, the <PyObject module="dagster.serdes"
object="ConfigurableClass" displayText="ConfigurableClass" />. This lets you specify what class to
instantiate for each of the instance's components (the `module` and `class` fields), as well as what
values to instantiate them with (the `config` field). Users can write their own classes to implement
any of this functionality and use the `dagster.yaml` file to tell Dagit to use the custom classes.

## Pipeline Run Configuration YAML

Along with instance-level configuration, Dagster supports configuring several things on a
per-pipeline-run basis. An example is shown below:

```yaml literalinclude caption=pipeline_run.yaml
file:src/pages/docs/deploying/pipeline_run.yaml
relativeToProject
```
