import { DynamicMetaTags } from 'components/MetaTags';
import { CodeReferenceLink } from 'components/CodeReference';

<DynamicMetaTags
  title="Dagster Types &amp; Expectations | Dagster"
  description="Dagster is a system for building modern data applications."
/>

import AnchorHeading from 'components/AnchorHeading';
import PyObject from 'components/PyObject';

# Dagster Types & Expectations

<CodeReferenceLink filePath="examples/docs_snippets/docs_snippets/intro_tutorial/basics/e04_quality/" />

## Dagster Types

We've seen how we can type the inputs and outputs of solids using Python 3's typing system, and how
to use Dagster's built-in config types, such as <PyObject module="dagster" object="String"
displayText="dagster.String" />, to define config schemas for our solids.

But what about when you want to define your own types?

This section will talk about the basics of Dagster's user defined types. You can also learn more
about typing in Dagster by reading [Dagster Types Guide](/learn/guides/dagster_types).

Let's look back at our simple `read_csv` solid.

```python literalinclude showLines startLine=7 caption=inputs_typed.py
file:/docs_snippets/docs_snippets/intro_tutorial/basics/e04_quality/inputs_typed.py
lines:7-15
```

The `lines` object returned by Python's built-in `csv.DictReader` is a list of
`collections.OrderedDict`, each of which represents one row of the dataset:

```python
[
    OrderedDict([
        ('name', '100% Bran'), ('mfr', 'N'), ('type', 'C'), ('calories', '70'), ('protein', '4'),
        ('fat', '1'), ('sodium', '130'), ('carbo', '5'), ('sugars', '6'), ('potass', '280'),
        ('vitamins', '25'), ('shelf', '3'), ('weight', '1'), ('cups', '0.33'),
        ('rating', '68.402973')
    ]),
    OrderedDict([
        ('name', '100% Natural Bran'), ('mfr', 'Q'), ('type', 'C'), ('calories', '120'),
        ('protein', '3'), ('fat', '5'), ('sodium', '15'), ('fiber', '2'), ('carbo', '8'),
        ('sugars', '8'), ('potass', '135'), ('vitamins', '0'), ('shelf', '3'), ('weight', '1'),
        ('cups', '1'), ('rating', '33.983679')
    ]),
    ...
]
```

This is a simple representation of a "data frame", or a table of data. We'd like to be able to use
Dagster's type system to type the output of `read_csv`, so that we can do type checking when we
construct the pipeline, ensuring that any solid consuming the output of `read_csv` expects to
receive a data frame.

To do this, we'll use the <PyObject module="dagster" object="DagsterType" displayText="DagsterType"
/> class:

```python literalinclude  showLines startLine=6 caption=custom_types.py
file:/docs_snippets/docs_snippets/intro_tutorial/basics/e04_quality/custom_types.py
lines:6-10
```

Now we can annotate the rest of our pipeline with our new type:

```python literalinclude showLines startLine=13 emphasize-lines=2,12 caption=custom_types.py
file:/docs_snippets/docs_snippets/intro_tutorial/basics/e04_quality/custom_types.py
lines:13-36
```

The type metadata now appears in dagit and the system will ensure the input and output to this solid
are indeed instances of `SimpleDataFrame`. As usual, run:

```bash
dagit -f custom_types.py
```

![custom_types_figure_one.png](/assets/images/tutorial/custom_types_figure_one.png)

You can see that the output of `read_csv` (which by default has the name `result`) is marked to be
of type `SimpleDataFrame`.

<br />

### Complex Type Checks

The Dagster framework will fail type checks when a value isn't an instance of the type we're
expecting, e.g., if `read_csv` were to return a `str` rather than a `SimpleDataFrame`.

Sometimes we know more about the types of our values, and we'd like to do deeper type checks. For
example, in the case of the `SimpleDataFrame`, we expect to see a list of OrderedDicts, and for each
of these OrderedDicts to have the same fields, in the same order.

The type check function allows us to do this:

```python literalinclude showLines startLine=7 emphasize-lines=1,21 caption=custom_types_2.py
file:/docs_snippets/docs_snippets/intro_tutorial/basics/e04_quality/custom_types_2.py
lines:7-30
```

Now, if our solid logic fails to return the right type, we'll see a type check failure. Let's
replace our `read_csv` solid with the following bad logic:

```python literalinclude showLines startLine=32 caption=custom_types_2.py
file:/docs_snippets/docs_snippets/intro_tutorial/basics/e04_quality/custom_types_2.py
lines:32-39
```

When we run the pipeline with this solid, we'll see an error like:

```bash
2020-06-15 14:08:00 - dagster - ERROR - custom_type_pipeline - 7fd9dce5-10e8-44ed-ba0c-0d48fdf9a5fc - STEP_FAILURE - Execution of step "bad_read_csv.compute" failed.
            cls_name = "AttributeError"
       error_message = "AttributeError: 'str' object has no attribute 'keys'\n"
               solid = "bad_read_csv"
    solid_definition = "bad_read_csv"
            step_key = "bad_read_csv.compute"
```

<br />

### Providing Input Values for Custom Types in Config

We saw earlier how, when a solid doesn't receive all of its inputs from other solids further
upstream in the pipeline, we can specify its input values in config:

```YAML literalinclude caption=inputs_env.yaml
file:/docs_snippets/docs_snippets/intro_tutorial/basics/e02_solids/inputs_env.yaml
```

The Dagster framework knows how to interpret values provided via config as scalar inputs. In this
case, `read_csv` just takes the string representation of the filepath from which it'll read a CSV.
But for more complex, custom types, we need to tell Dagster how to interpret config values.

Consider our `LessSimpleDataFrame`. It might be convenient if Dagster knew automatically how to read
a data frame in from a CSV file, without us needing to separate that logic into the `read_csv`
solid&mdash;especially if we knew the provenance and format of that CSV file (e.g., if we were using
standard CSVs as an internal interchange format) and didn't need the full configuration surface of a
general purpose `read_csv` solid.

What we want to be able to do is write:

```YAML literalinclude caption=custom_type_input.yaml
file:/docs_snippets/docs_snippets/intro_tutorial/basics/e04_quality/custom_type_input.yaml
```

In order for the Dagster machinery to be able to decode the config value
`{'csv': 'cereal.csv'}` into an input of the correct
`LessSimpleDataFrame` value, we need to write what we call a **type loader**.

```python literalinclude showLines startLine=32 caption=custom_types_3.py
file:/docs_snippets/docs_snippets/intro_tutorial/basics/e04_quality/custom_types_3.py
lines:32-39
```

A function decorated with <PyObject module="dagster" object="dagster_type_loader" displayText="@dagster_type_loader" /> should
take the context object, as usual, and a parameter
representing the parsed config field. The schema for this field is
defined by the argument to the <PyObject module="dagster" object="dagster_type_loader" displayText="@dagster_type_loader" /> decorator.

Here, we introduce the <PyObject module="dagster" object="Selector" /> type, which lets you specify
mutually exclusive options in config schemas. Here, there's only one option, `csv`, but you can
imagine a more sophisticated data frame type that might also know how to load its inputs from
other formats and sources, and might have a selector with fields like `parquet`, `xlsx`, `sql`, etc.

Then insert this into the original declaration:

```python literalinclude showLines startLine=42 emphasize-lines=5 caption=custom_types_3.py
file:/docs_snippets/docs_snippets/intro_tutorial/basics/e04_quality/custom_types_3.py
lines:42-47
```

Now if you run a pipeline with this solid from dagit you will be able to
provide sources for these inputs via config:

```python literalinclude showLines startLine=70 caption=custom_types_3.py
file:/docs_snippets/docs_snippets/intro_tutorial/basics/e04_quality/custom_types_3.py
lines:70-80
```

<br />

### Testing Custom Types

As you write your own custom types, you'll also want to set up unit
tests that ensure your types are doing what you expect them to. Dagster
includes a utility function, <PyObject module="dagster" object="check_dagster_type" />,
that lets you type check any Dagster type against any value.

```python literalinclude showLines startLine=102 caption=custom_types_test.py
file:/docs_snippets/docs_snippets/intro_tutorial/basics/e04_quality/custom_types_test.py
lines:102-114
```

Well tested library types can be reused across solids and pipelines to
provide standardized type checking within your organization's data
applications.

<br />

### MyPy Compliance

**Note:** _this section requires Python 3._

In cases where DagsterTypes are created that do not have corresponding usable Python types, and the
user wishes to remain mypy compliant, there are two options.

One is using `InputDefinition` and `OutputDefinition` exclusively for Dagster types, and reserving
type annotations for naked Python types _only_. This is verbose, but is explicit and clear.

```python literalinclude showLines startLine=21 emphasize-lines=1-5,14-15 caption=custom_types_mypy_verbose.py
file:/docs_snippets/docs_snippets/intro_tutorial/basics/e04_quality/custom_types_mypy_verbose.py
lines:21-47
```

If one wishes to use type annotations exclusively but still use Dagster types without a 1:1 Python
type counterpart, the typechecking behavior must be modified. For this we recommend using the
`typing.TYPE_CHECKING` property in the Python typing module.

While inelegant, this centralizes boilerplate to the type instantiation, rather than having it on all
places where the type is referenced.

```python literalinclude showLines startLine=7 emphasize-lines=1-8 caption=custom_types_mypy_typing_trick.py
file:/docs_snippets/docs_snippets/intro_tutorial/basics/e04_quality/custom_types_mypy_typing_trick.py
lines:7-25
```

<br />

### Metadata and Custom Type Checks

Custom types can also yield metadata about the type check. For example, in the case of our data
frame, we might want to record the number of rows and columns in the dataset when our type checks
succeed, and provide more information about why type checks failed when they fail. User-defined type
check functions can optionally return a <PyObject module="dagster" object="TypeCheck" /> object that
contains metadata about the success or failure of the type check. Let's see how to use this to emit
some summary statistics about our DataFrame type:

```python literalinclude showLines startLine=18 emphasize-lines=3-9,33-53 caption=custom_types_4.py
file:/docs_snippets/docs_snippets/intro_tutorial/basics/e04_quality/custom_types_4.py
lines:18-71
```

A <PyObject module="dagster" object="TypeCheck" displayText="TypeCheck" /> must include a `success`
argument describing whether the check passed or failed, and may include a description and/or a list
of <PyObject module="dagster" object="EventMetadataEntry" /> objects. You should use the static
constructors on <PyObject module="dagster" object="EventMetadataEntry" /> to construct these
objects, which are flexible enough to support arbitrary metadata in JSON or Markdown format.

Dagit knows how to display and archive structured metadata of this kind for future review:

![custom_types_figure_two.png](/assets/images/tutorial/custom_types_figure_two.png)

<br />

## Expectations

Custom type checks and metadata are appropriate for checking that a value will behave as we expect,
and for collecting summary information about values.

But sometimes we want to make more specific, data- and business logic-dependent assertions about the
semantics of values. It typically isn't appropriate to embed assertions like these into data types
directly.

For one, they will usually vary substantially between instantiations&mdash;for example, we don't
expect all data frames to have the same number of columns, and over-specifying data types (e.g.,
`SixColumnedDataFrame`) makes it difficult to write logic that works generically (e.g., over all data frames).

What's more, these additional, deeper semantic assertions are often non-stationary. Typically,
you'll start running a pipeline with certain expectations about the data that you'll see; but over
time, you'll learn more about your data (making your expectations more precise), and the process
that generates your data will shift (making some of your expectations invalid).

We've already encountered the <PyObject module="dagster" object="TypeCheck" /> event, which is
typically yielded by the type machinery (but can also be yielded manually from the body of a solid's
compute function); <PyObject module="dagster" object="ExpectationResult" /> is another kind of
structured side-channel result that a solid can yield. These extra events don't get passed to
downstream solids and they aren't used to define the data dependencies of a pipeline DAG.

```python literalinclude showLines startLine=93 emphasize-lines=1-3,31 caption=custom_types_bad_5.py
file:/docs_snippets/docs_snippets/intro_tutorial/basics/e04_quality/custom_types_bad_5.py
lines:93-133
```

Until now, every solid we've encountered has returned its result value, or `None`. But solids can
also yield events of various types for side-channel communication about the results of their
computations.

Running this pipeline yields an <PyObject module="dagster" object="ExpectationResult" /> with
`success` set to `False` since we expect entries in the `calories` column to be of type `int` but
they are of type `string`. We note that this precedes our incorrect result that the least caloric
cereal is Corn Flakes (100 calories per serving) and the most caloric cereal Strawberry Fruit Wheats
(90 calories per serving).

![custom_types_bad_data.png](/assets/images/tutorial/custom_types_bad_data.png)

To fix this, we can cast `calories` to `int` during the loading process:

```python literalinclude showLines startLine=75 emphasize-lines=7 caption=custom_types_5.py
file:/docs_snippets/docs_snippets/intro_tutorial/basics/e04_quality/custom_types_5.py
lines:75-86
```

Running this pipeline yields an <PyObject module="dagster" object="ExpectationResult" /> with
`success` set to `True` and the correct result that the least caloric cereal is All-Bran with Extra
Fiber (50 calories per serving) and the most caloric cereal is Mueslix Crispy Blend (160 calories
per serving).

This part of this system remains relatively immature, but yielding structured expectation results
from your solid logic means that in future, tools like Dagit will be able to aggregate and track
expectation results, as well as implement sophisticated policy engines to drive alerting and
exception handling on a deep semantic basis. You can learn more about it by reading [Data Quality
Tests Guide](/learn/guides/testing/expectations).

<br />

## Conclusion

🎉 Congratulations! Having reached this far, you now have a working, testable, and maintainable data
pipeline. You’ve also learned the basics of Dagster, and you should now be able to build your own
data applications using Dagster!

<br />

<br />
