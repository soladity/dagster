---
title: Dagster with Airflow | Dagster
description: The dagster-airflow package allows you to export Dagster pipelines as Airflow DAGs, as well as to import Airflow DAGs into Dagster pipelines.
---

# Using Dagster with Airflow

<CodeReferenceLink filePath="examples/airflow_ingest" />

The [`dagster-airflow`](/\_apidocs/libraries/dagster_airflow) package allows you to export Dagster pipelines as Airflow DAGs, as well as to import Airflow DAGs into Dagster pipelines.

Dagster is a fully-featured orchestrator and does not require a system like Airflow to deploy, execute, or schedule pipelines. The main scenarios for using Dagster with Airflow are:

- You have an existing Airflow setup that's too difficult to migrate away from, but you want to use Dagster for local development.
- You want to migrate from Airflow to Dagster in an incremental fashion.

## Exporting Dagster pipelines to Airflow

You can compile Dagster pipelines into DAGs that can be understood by Airflow. Each solid in the pipeline becomes and Airflow task. Here's a pipeline defined using Dagster:

```python file=/integrations/airflow/hello_cereal.py
import csv

import requests
from dagster import ModeDefinition, fs_io_manager, pipeline, solid


@solid
def download_cereals():
    response = requests.get("https://docs.dagster.io/assets/cereal.csv")
    lines = response.text.split("\n")
    return [row for row in csv.DictReader(lines)]


@solid
def find_sugariest(context, cereals):
    sorted_by_sugar = sorted(cereals, key=lambda cereal: cereal["sugars"])
    context.log.info(f'{sorted_by_sugar[-1]["name"]} is the sugariest cereal')


@pipeline(mode_defs=[ModeDefinition(resource_defs={"io_manager": fs_io_manager})])
def hello_cereal_pipeline():
    find_sugariest(download_cereals())
```

Notice that this pipeline uses a mode with the <PyObject module="dagster" object="fs_io_manager" />. This is important because each Airflow task executes in its own process. Therefore, we need to write outputs to a persistent <PyObject module="dagster" object="IOManager" />, so that downstream tasks can access them, instead of fetching them from memory, which is Dagster's default.

To make this pipeline available inside Airflow, you can write an an Airflow DAG definition file that invokes <PyObject module="dagster_airflow" object="make_airflow_dag" />:

```python file=/integrations/airflow/hello_cereal_dag.py
"""
The airflow DAG for hello_cereal_pipeline
"""
import datetime

from dagster_airflow.factory import make_airflow_dag

DEFAULT_ARGS = {
    "owner": "airflow",
    "depends_on_past": False,
    "start_date": datetime.datetime(2019, 11, 7),
    "email": ["airflow@example.com"],
    "email_on_failure": False,
    "email_on_retry": False,
}

dag, tasks = make_airflow_dag(
    module_name="docs_snippets.integrations.airflow.hello_cereal",
    pipeline_name="hello_cereal_pipeline",
    dag_kwargs={"default_args": DEFAULT_ARGS, "max_active_runs": 1},
)
```

If you run this code interactively, you'll see that `dag` and `tasks` are ordinary Airflow objects, just as you'd expect to see when defining an Airflow pipeline manually:

```python
>>> dag
<DAG: hello_cereal_pipeline>
>>> tasks
[<Task(DagsterPythonOperator): hello_cereal>]
```

Like other Airflow DAG definition files, this should go inside `$AIRLFLOW_HOME/dags`. The `docs_snippets.integrations.airflow.hello_cereal` module that's passed as the value for the `module_name` argument must be importable via the sys.path.

After this, the DAG should show up inside Airflow:

<Image
alt="intro_airflow_one.png"
src="/images/deploying/intro_airflow_one.png"
width={1272}
height={868}
/>

### Running Containerized

The approach above runs each solid inside an operator that's similar to the Airflow PythonOperator. If you instead want to containerize your Dagster pipeline and run it using an operator that's similar to the Airflow DockerOperator, you can use <PyObject module="dagster_airflow" object="make_airflow_dag_containerized" />.

As in the uncontainerized case, you'll put a new Python file defining your DAG in the directory in which Airflow looks for DAGs:

```python file=/integrations/airflow/containerized.py
"""
The containerized airflow DAG for hello_cereal_pipeline
"""

import datetime

from dagster_airflow.factory import make_airflow_dag_containerized

DEFAULT_ARGS = {
    "owner": "airflow",
    "depends_on_past": False,
    "start_date": datetime.datetime(2019, 11, 7),
    "email": ["airflow@example.com"],
    "email_on_failure": False,
    "email_on_retry": False,
}

dag, steps = make_airflow_dag_containerized(
    module_name="docs_snippets.integrations.airflow.hello_cereal",
    pipeline_name="hello_cereal_pipeline",
    image="dagster-airflow-demo-repository",
    dag_kwargs={"default_args": DEFAULT_ARGS, "max_active_runs": 1},
)
```

The `image` argument is the name of the Docker image. Running in a containerized context requires a persistent intermediate storage layer available to the Dagster containers, such as a network filesystem, S3, or GCS. You can pass `op_kwargs` through to the the **`DagsterDockerOperator`** to use custom TLS settings, the private registry of your choice, etc., just as you would configure the ordinary Airflow **`DockerOperator`**.

If you want your containerized pipeline to be available to Airflow operators running on other machines (for example, in environments where Airflow workers are running remotely) you'll need to push your Docker image to a Docker registry so that remote instances of Docker can pull the image by name, or otherwise ensure that the image is available on remote nodes.

## Ingesting DAGs from Airflow

This example demonstrates how to use <PyObject module="dagster_airflow" object="make_dagster_pipeline_from_airflow_dag" /> to compile an Airflow DAG into a Dagster pipeline that can be executed (and explored) the same way as a Dagster-native pipeline.

There are two pipelines in the repo:

- `airflow_simple_dag` demonstrates the use of Airflow templates.
- `airflow_complex_dag` shows the translation of a more complex dependency structure.

```python file=../../airflow_ingest/repo.py startafter=start_repo_marker_0 endbefore=end_repo_marker_0
from airflow_ingest.airflow_complex_dag import complex_dag
from airflow_ingest.airflow_simple_dag import simple_dag
from dagster import repository
from dagster_airflow.dagster_pipeline_factory import make_dagster_pipeline_from_airflow_dag

airflow_simple_dag = make_dagster_pipeline_from_airflow_dag(simple_dag)
airflow_complex_dag = make_dagster_pipeline_from_airflow_dag(complex_dag)


@repository
def airflow_ingest_example():
    return [airflow_complex_dag, airflow_simple_dag]
```

Note that the "execution_date" for the Airflow DAG is specified through the pipeline tags. To specify tags, call to:

```python
make_dagster_pipeline_from_airflow_dag(
    dag=dag,
    tags={'airflow_execution_date': utc_execution_date_str}
)
```
