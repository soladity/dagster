---
title: Dagster with Great Expectations | Dagster
---

# Using Dagster with Great Expectations

<CodeReferenceLink filePath="examples/ge_example" />

This example demonstrates how to use the GE op factory [`dagster-ge`](/\_apidocs/libraries/dagster_ge) to test incoming data against a set of expectations built through [Great Expectations](https://docs.greatexpectations.io/en/latest/)' tooling.

For this example, we'll be using two versions of a dataset of baseball team payroll and wins, with one version modified to hold incorrect data.

You can use <PyObject module="dagster_ge" object="ge_validation_op_factory" /> to generate Dagster ops that integrate with Great Expectations. For example, here we show a basic call to this GE op factory, with two required arguments: `datasource_name` and expectation `suite_name`.

```python file=../../ge_example/ge_example/ge_demo.py startafter=start_ge_demo_marker_factory endbefore=end_ge_demo_marker_factory
payroll_expectations = ge_validation_op_factory(
    name="ge_validation_op", datasource_name="getest", suite_name="basic.warning"
)
```

The GE validations will happen inside the ops created above. Each of the ops will yield an <PyObject object="ExpectationResult" /> with a structured dict of metadata from the GE suite. The structured metadata contain both summary stats from the suite and expectation by expectation results. The op will output the full result in case you want to process it differently. Here's how other ops could use the full result, where `expectation` is the result:

```python file=../../ge_example/ge_example/ge_demo.py startafter=start_ge_demo_marker_op endbefore=end_ge_demo_marker_op
@op
def postprocess_payroll(numrows, expectation):
    if expectation["success"]:
        return numrows
    else:
        raise ValueError
```

You can configure the GE Data Context via the `ge_data_context` resource from `dagster-ge` integration package. All we need to do to expose GE to Dagster is to provide the root of the GE directory (the path to the great_expectations file on your machine).

Finally, here's the full job definition using the GE op, with a default run configuration to use the correct set of data:

```python file=../../ge_example/ge_example/ge_demo.py startafter=start_ge_demo_marker_job endbefore=end_ge_demo_marker_job
@job(
    resource_defs={"ge_data_context": ge_data_context},
    config={
        "resources": {
            "ge_data_context": {
                "config": {"ge_root_dir": file_relative_path(__file__, "./great_expectations")}
            }
        },
        "solids": {
            "read_in_datafile": {
                "inputs": {
                    "csv_path": {"value": file_relative_path(__file__, "./data/succeed.csv")}
                }
            }
        },
    },
)
def payroll_data():
    output_df = read_in_datafile()

    postprocess_payroll(process_payroll(output_df), payroll_expectations(output_df))
```

We can see that we can easily swap the path for `succeed.csv` with `fail.csv` to exercise our job with incorrect data.
