---
title: Graphs | Dagster
description: A graph is a set of ops or sub-graphs with explicit data dependencies on each other.
---

# Graphs

A graph is a set of ops or sub-graphs with explicit data dependencies on each other.

<!-- TODO: Update image to refer to graphs and ops -->

<Image
alt="Graph Diagram"
src="/images/pipelines.png"
width={3200}
height={1040}
/>

## Relevant APIs

| Name                                                   | Description                                                                                                                                                                               |
| ------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| <PyObject module="dagster" object="graph" decorator /> | The decorator used to define a graph.                                                                                                                                                     |
| <PyObject module="dagster" object="GraphDefinition" /> | Class for graphs. You almost never want to use initialize this class directly. Instead, you should use the <PyObject object="graph" decorator /> which returns an instance of this class. |

## Overview

Ops are linked together into graphs by defining the dependencies between their inputs and outputs. An important difference between Dagster and other workflow systems is that in Dagster, op dependencies are expressed as data dependencies, not just execution dependencies. Graphs can also contain other graphs.

This difference enables Dagster to support richer modeling of dependencies. Instead of merely ensuring that the order of execution is correct, dependencies in Dagster provide a variety of compile and run-time checks.

Like ops, graphs are "logical" entities - i.e. the same graph is meant to be able to be executed in multiple environments, and you can provide [configuration](/concepts/configuration/config-schema) and [resources](/concepts/resources) to specialize them to particular environments. A [job](/concepts/solids-pipelines/jobs) is an entity that combines a graph with resources and configuration.  After packaging a graph as a job, you can define a [schedule](/concepts/partitions-schedules-sensors/schedules) to execute it at a fixed interval or define a [sensor](/concepts/partitions-schedules-sensors/sensors) to trigger it when external changes occur. You can also launch it [manually from Dagit, GraphQL APIs, or the command line](/concepts/solids-pipelines/pipeline-execution).

## Defining a graph

To define a graph, use the <PyObject object="graph" decorator /> decorator.

Within the decorated function body, you can use function calls to indicate the dependency structure between the ops and sub-graphs making up the graph.

In this example, the `add_one` op depends on the `return_one` op's output. Because this data dependency exists, the `add_one` op executes after `return_one` runs successfully and emits the required output.

```python file=/concepts/solids_pipelines/pipelines.py startafter=start_pipeline_example_marker endbefore=end_pipeline_example_marker
@op
def return_one(context):
    return 1


@op
def add_one(context, number: int):
    return number + 1


@graph
def one_plus_one():
    add_one(return_one())
```

## Aliases and Tags

### Op aliases

You can use the same op definition multiple times in the same graph.

```python file=/concepts/solids_pipelines/pipelines.py startafter=start_multiple_usage_pipeline endbefore=end_multiple_usage_pipeline
@graph
def multiple_usage():
    add_one(add_one(return_one()))
```

To differentiate between the two invocations of `add_one`, Dagster automatically aliases the op names to be `add_one` and `add_one_2`.

You can also manually define the alias by using the `.alias` method on the op invocation.

```python file=/concepts/solids_pipelines/pipelines.py startafter=start_alias_pipeline endbefore=end_alias_pipeline
@graph
def alias():
    add_one.alias("second_addition")(add_one(return_one()))
```

### Op Tags

Similar to aliases, you can also define op tags on an op invocation.

```python file=/concepts/solids_pipelines/pipelines.py startafter=start_tag_pipeline endbefore=end_tag_pipeline
@graph
def tagged_add_one():
    add_one.tag({"my_tag": "my_value"})(add_one(return_one()))
```

### Graph Tags

Graphs can specify a set of tags that are also automatically set on the resulting runs.

```python file=/concepts/solids_pipelines/pipelines.py startafter=start_tags_pipeline endbefore=end_tags_pipeline
@graph(tags={"my_tag": "my_value"})
def my_tags_pipeline():
    my_op()
```

This graph defines a `my_tag` tag. Any runs created using this graph will also have the same tag.

## Examples

There are many DAG structures can be represented using graphs. This section covers a few basic patterns you can use to build more complex graphs.

### Linear Dependencies

The simplest graph structure is the linear graph. We return one output from the root op, and pass along data through single inputs and outputs.

<Image
alt="Linear Graph"
src="/images/concepts/pipelines/linear.png"
width={1000}
height={250}
/>

```python file=/concepts/solids_pipelines/linear_pipeline.py startafter=start_marker endbefore=end_marker
from dagster import graph, op


@op
def return_one(context) -> int:
    return 1


@op
def add_one(context, number: int) -> int:
    return number + 1


@graph
def linear():
    add_one(add_one(add_one(return_one())))
```

### Multiple Inputs

<Image
alt="Multiple Inputs"
src="/images/concepts/pipelines/multi-inputs.png"
width={1000}
height={250}
/>

A single output can be passed to multiple inputs on downstream ops. In this example, the output from the first op is passed to two different ops. The outputs of those ops are combined and passed to the final op.

```python file=/concepts/solids_pipelines/multiple_io_pipeline.py startafter=start_marker endbefore=end_marker
from dagster import graph, op


@op
def return_one(context) -> int:
    return 1


@op
def add_one(context, number: int):
    return number + 1


@op
def adder(context, a: int, b: int) -> int:
    return a + b


@graph
def inputs_and_outputs():
    value = return_one()
    a = add_one(value)
    b = add_one(value)
    adder(a, b)
```

### Conditional Branching

<Image
alt="Conditional Branch"
src="/images/concepts/pipelines/conditional.png"
width={1000}
height={250}
/>

An op only starts to execute once all of its inputs have been resolved. We can use this behavior to model conditional execution of ops.

In this example, the `branching_op` outputs either the `branch_1` result or `branch_2` result. Since op execution is skipped for ops that have unresolved inputs, only one of the downstream ops will execute.

```python file=/concepts/solids_pipelines/branching_pipeline.py startafter=start_marker endbefore=end_marker
import random

from dagster import Out, Output, graph, op


@op(out={"branch_1": Out(is_required=False), "branch_2": Out(is_required=False)})
def branching_op():
    num = random.randint(0, 1)
    if num == 0:
        yield Output(1, "branch_1")
    else:
        yield Output(2, "branch_2")


@op
def branch_1_op(_input):
    pass


@op
def branch_2_op(_input):
    pass


@graph
def branching():
    branch_1, branch_2 = branching_op()
    branch_1_op(branch_1)
    branch_2_op(branch_2)
```

### Fixed Fan-in Graph

<Image
alt="Fixed Fan-in"
src="/images/concepts/pipelines/fixed-fan-in.png"
width={1000}
height={250}
/>

If you have a fixed set of op that all return the same output type, you can collect all the outputs into a list and pass them into a single downstream op.

The downstream op executes only if all of the outputs were successfully created by the upstream op.

```python file=/concepts/solids_pipelines/fan_in_pipeline.py startafter=start_marker endbefore=end_marker
from typing import List

from dagster import graph, op


@op
def return_one() -> int:
    return 1


@op
def sum_fan_in(nums: List[int]) -> int:
    return sum(nums)


@graph
def fan_in():
    fan_outs = []
    for i in range(0, 10):
        fan_outs.append(return_one.alias(f"return_one_{i}")())
    sum_fan_in(fan_outs)
```

In this example, we have 10 op that all output the number `1`. The `sum_fan_in` op takes all of these outputs as a list and sums them.

### Dynamic Mapping & Collect

In most cases, the structure of a graph is pre-determined before execution. Dagster also has support for creating graphs where the final structure is not determined until run-time. This is useful for graph structures where you want to execute a separate instance of an op for each entry in a certain output.

In this example, we have an op `files_in_directory` that defines a <PyObject object="DynamicOut" />. We `map` over the dynamic output which will cause the downstream dependencies to be cloned for each <PyObject object="DynamicOutput" /> that is yielded. The downstream copies can be identified by the `mapping_key` supplied to <PyObject object="DynamicOutput"/>. Once that's all complete, we `collect` over the results of `process_file` and pass that in to `summarize_directory`.

```python file=/concepts/solids_pipelines/dynamic_pipeline/dynamic_pipeline.py startafter=start_marker endbefore=end_marker
import os
from typing import List

from dagster import DynamicOut, DynamicOutput, Field, graph, op
from dagster.utils import file_relative_path


@op(
    config_schema={"path": Field(str, default_value=file_relative_path(__file__, "sample"))},
    out=DynamicOut(str),
)
def files_in_directory(context):
    path = context.op_config["path"]
    dirname, _, filenames = next(os.walk(path))
    for file in filenames:
        yield DynamicOutput(
            value=os.path.join(dirname, file),
            # create a mapping key from the file name
            mapping_key=file.replace(".", "_").replace("-", "_"),
        )


@op
def process_file(path: str) -> int:
    # simple example of calculating size
    return os.path.getsize(path)


@op
def summarize_directory(sizes: List[int]) -> int:
    # simple example of totalling sizes
    return sum(sizes)


@graph
def process_directory():
    file_results = files_in_directory().map(process_file)
    summarize_directory(file_results.collect())
```

### Order-based Dependencies (Nothing dependencies)

Dependencies in Dagster are primarily _data dependencies_. Using data dependencies means each input of an op depends on the output of an upstream op.

If you have an op, say `Op A`, that does not depend on any outputs of another op, say `Op B`, there theoretically shouldn't be a reason for `Op A` to run after `Op B`. In most cases, these two ops should be parallelizable. However, there are some cases where an explicit ordering is required, but it doesn't make sense to pass data through inputs and outputs to model the dependency.

If you need to model an explicit ordering dependency, you can use the <PyObject object="Nothing"/> Dagster type on the input definition of the downstream op. This type specifies that you are passing "nothing" via Dagster between the ops, while still uses inputs and outputs to model the dependency between the two ops.

```python file=/concepts/solids_pipelines/order_based_dependency_pipeline.py startafter=start_marker endbefore=end_marker
from dagster import In, Nothing, graph, op


@op
def create_table_1():
    get_database_connection().execute("create table_1 as select * from some_source_table")


@op(ins={"start": In(Nothing)})
def create_table_2():
    get_database_connection().execute("create table_2 as select * from table_1")


@graph
def nothing_dependency():
    create_table_2(start=create_table_1())
```

In this example, `create_table_2` has an input of type `Nothing` meaning that it doesn't exepct any data to be provided by the upstream op. This lets us connect them in the graph definition so that `create_table_2` executes only after `create_table_1` successfully executes.

`Nothing` type inputs do not have a corresponding parameter in the function since there is no data to pass. When connecting the dependencies, it is recommended to use keyword args to prevent mix-ups with other positional inputs.

Note that in most cases, it is usually possible to pass some data dependency. In the example above, even though we probably don't want to pass the table data itself between the ops, we could pass table pointers. For example, `create_table_1` could return a `table_pointer` output of type `str` with a value of `table_1`, and this table name can be used in `create_table_2` to more accurately model the data dependency.

Dagster also provides more advanced abstractions to handle dependencies and IO. If you find that you are finding it difficult to model data dependencies when using external storages, check out [IOManagers](/concepts/io-management/io-managers).

## Patterns

### Constructing GraphDefinitions

You may run into a situation where you need to programmatically construct the dependencies for a graph. In that case, you can directly define the <PyObject module="dagster" object="GraphDefinition"/> object.

To construct a GraphDefinition, you need to pass the constructor a graph name, a list of op or graph definitions, and a dictionary defining the dependency structure. The dependency structure declares the dependencies of each op’s inputs on the outputs of other ops in the graph. The top-level keys of the dependency dictionary are the string names of ops or graphs. If you are using op aliases, be sure to use the aliased name. Values of the top-level keys are also dictionary, which maps input names to a <PyObject object="DependencyDefinition"/>.

```python file=/concepts/solids_pipelines/pipelines.py startafter=start_pipeline_definition_marker endbefore=end_pipeline_definition_marker
one_plus_one_graph_def = GraphDefinition(
    name="one_plus_one",
    node_defs=[return_one, add_one],
    dependencies={"add_one": {"number": DependencyDefinition("return_one")}},
)
```

### Graph DSL

Sometimes you may want to construct the dependencies of a graph definition from a YAML file or similar. This is useful when migrating to Dagster from other workflow systems.

For example, you can have a YAML like this:

```YAML file=/concepts/solids_pipelines/my_graph.yaml
name: some_example
description: blah blah blah
ops:
  - def: add_one
    alias: A
  - def: add_one
    alias: B
    deps:
      num:
        op: A
  - def: add_two
    alias: C
    deps:
      num:
        op: A
  - def: subtract
    deps:
      left:
        op: B
      right:
        op: C
```

You can programatically generate a PipelineDefinition from this YAML:

```python file=/concepts/solids_pipelines/dep_dsl.py
import os

from dagster import DependencyDefinition, GraphDefinition, SolidInvocation, op
from dagster.utils.yaml_utils import load_yaml_from_path


@op
def add_one(num: int) -> int:
    return num + 1


@op
def add_two(num: int) -> int:
    return num + 2


@op
def subtract(left: int, right: int) -> int:
    return left + right


def construct_graph_with_yaml(yaml_file, op_defs) -> GraphDefinition:
    yaml_data = load_yaml_from_path(yaml_file)

    deps = {}

    for op_yaml_data in yaml_data["ops"]:
        def_name = op_yaml_data["def"]
        alias = op_yaml_data.get("alias", def_name)
        op_deps_entry = {}
        for input_name, input_data in op_yaml_data.get("deps", {}).items():
            op_deps_entry[input_name] = DependencyDefinition(
                solid=input_data["op"], output=input_data.get("output", "result")
            )
        deps[SolidInvocation(name=def_name, alias=alias)] = op_deps_entry

    return GraphDefinition(
        name=yaml_data["name"],
        description=yaml_data.get("description"),
        node_defs=op_defs,
        dependencies=deps,
    )


def define_dep_dsl_graph() -> GraphDefinition:
    path = os.path.join(os.path.dirname(__file__), "my_graph.yaml")
    return construct_graph_with_yaml(path, [add_one, add_two, subtract])
```
