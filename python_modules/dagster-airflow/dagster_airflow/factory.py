import datetime
import json

from yaml import dump

from airflow import DAG
from airflow.operators.python_operator import PythonOperator

from dagster import check, PipelineDefinition, RepositoryDefinition
from dagster.core.execution import create_execution_plan

from dagit.app import RepositoryContainer
from dagit.cli import execute_query_from_cli

from .dagster_plugin import QUERY_TEMPLATE
from .compile import coalesce_execution_steps
from .scaffold import format_config_for_graphql
from .utils import IndentingBlockPrinter


DEFAULT_ARGS = {
    'depends_on_past': False,
    'email': ['airflow@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'owner': 'airflow',
    'retries': 1,
    'retry_delay': datetime.timedelta(0, 300),
    'start_date': datetime.datetime(1900, 1, 1, 0, 0),
}


def _make_dag_description(pipeline_name, env_config):
    with IndentingBlockPrinter() as printer:
        printer.block(
            '\'\'\'Editable scaffolding autogenerated by dagster-airflow from pipeline '
            '{pipeline_name} with config:'.format(pipeline_name=pipeline_name)
        )
        printer.blank_line()

        with printer.with_indent():
            for line in dump(env_config).split('\n'):
                printer.line(line)
        printer.blank_line()

        return printer.read()


def _make_python_callable(dag_id, pipeline, env_config, step_keys):
    repository = RepositoryDefinition('<<ephemeral repository>>', {dag_id: lambda: pipeline})
    repository_container = RepositoryContainer(repository=repository)

    config = format_config_for_graphql(env_config)

    def python_callable(**kwargs):
        run_id = kwargs.get('dag_run').run_id
        query = QUERY_TEMPLATE.format(
            config=config,
            run_id=run_id,
            step_keys=json.dumps(step_keys),
            pipeline_name=pipeline.name,
        )
        return execute_query_from_cli(repository_container, query, variables=None)

    return python_callable


def make_airflow_dag(pipeline, env_config=None, dag_id=None, dag_description=None, dag_kwargs=None):
    check.inst_param(pipeline, 'pipeline', PipelineDefinition)
    env_config = check.opt_dict_param(env_config, 'env_config', key_type=str)
    pipeline_name = pipeline.name
    dag_id = check.opt_str_param(dag_id, 'dag_id', pipeline.name)
    dag_description = check.opt_str_param(
        dag_description, 'dag_description', _make_dag_description(pipeline_name, env_config)
    )
    dag_kwargs = dict(
        {'default_args': DEFAULT_ARGS},
        **check.opt_dict_param(dag_kwargs, 'dag_kwargs', key_type=str)
    )

    dag = DAG(dag_id=dag_id, description=dag_description, **dag_kwargs)

    execution_plan = create_execution_plan(pipeline, env_config)

    tasks = {}

    coalesced_plan = coalesce_execution_steps(execution_plan)

    for solid_name, solid_steps in coalesced_plan.items():

        step_keys = [step.key for step in solid_steps]

        task = PythonOperator(
            task_id=solid_name,
            provide_context=True,
            python_callable=_make_python_callable(dag_id, pipeline, env_config, step_keys),
            dag=dag,
        )

        tasks[solid_name] = task

        for solid_step in solid_steps:
            for step_input in solid_step.step_inputs:
                prev_solid_name = execution_plan.get_step_by_key(
                    step_input.prev_output_handle.step_key
                ).tags['solid']
                if solid_name != prev_solid_name:
                    tasks[prev_solid_name].set_downstream(task)

    return (dag, [tasks[solid_name] for solid_name in coalesced_plan.keys()])
