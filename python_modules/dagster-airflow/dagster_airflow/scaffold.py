# What we want to do here is consume a pipeline definition and a config, and then generate an
# Airflow DAG definition, each of whose nodes corresponds to one step of the execution plan.

import os

try:
    from black import format_file_contents

    BLACK = True
except ImportError:
    from yapf.yapflib.yapf_api import FormatCode

    BLACK = False

from datetime import datetime, timedelta

from airflow import DAG
from dagster import check, PipelineDefinition
from dagster.core.execution import create_execution_plan

from .storage import Storage

# Use a shared Storage for s3 access
# need to pass requirements to and from each step


def normalize_key(key):
    return key.replace('_', '__').replace('.', '_')


# dag = DAG(
#     'minimal_dockerized_dagster_airflow_demo',
#     default_args=default_args,
#     schedule_interval=timedelta(minutes=10),
# )

# t1 = DagsterOperator(
#     api_version='1.21',
#     docker_url=os.getenv('DOCKER_HOST'),
#     command='pipeline execute demo_pipeline -e env.yml',
#     image='dagster-airflow-demo:latest',
#     network_mode='bridge',
#     task_id='minimal_dockerized_dagster_airflow_node',
#     dag=dag,
#     host_tmp_dir='/tmp/airflow',
#     tmp_dir='/tmp/airflow',
#     tls_client_ert=os.getenv('DOCKER_CERT_PATH'),
# )

DEFAULT_ARGS = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime.utcnow(),
    'email': ['airflow@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

LINE_LENGTH = 100

BLACK_KWARGS = {'line_length': LINE_LENGTH, 'fast': True}

DAG_SCAFFOLD = """
'''Autogenerated by dagster-airflow from pipeline {pipeline_name} with env_config:

{printed_env_config}
'''

import datetime

from airflow import DAG
from airflow.operators.dagster_plugin import DagsterOperator

# Set your S3 connection id here, if you do not want to use the default `aws_default` connection
S3_CONN_ID = 'aws_default'

dag = DAG(
    dag_id='{pipeline_name}',
    description='{pipeline_description}',
    default_args={default_args}{dag_kwargs},
)

{step_definitions}

{step_dependencies}
"""


def scaffold_airflow_dag(
    pipeline, env_config, image, output_path=None, storage=Storage, **dag_kwargs
):
    check.inst_param(pipeline, 'pipeline', PipelineDefinition)
    check.opt_dict_param(env_config, 'env_config', key_type=str)

    output_path = os.path.abspath(os.path.expanduser(output_path))

    execution_plan = create_execution_plan(pipeline, env_config)

    default_args = dict(DEFAULT_ARGS, **(dag_kwargs.pop('default_args', {})))

    dag_file = DAG_SCAFFOLD.format(
        pipeline_name=pipeline.name,
        pipeline_description='***Autogenerated by dagster-airflow***'
        + (
            '\\n{description}'.format(description=pipeline.description)
            if pipeline.description
            else ''
        ),
        printed_env_config=(
            format_file_contents(str(env_config), **BLACK_KWARGS)
            if BLACK
            else FormatCode(str(env_config))[0]
        ),
        default_args=str(default_args),
        dag_kwargs=(
            ',\n    '
            + '\n    '.join(
                [
                    '{key}={str_value}'.format(key=key, str_value=str(value))
                    for key, value in dag_kwargs.items()
                ]
            )
            if dag_kwargs
            else ''
        ),
        step_definitions='\n'.join(
            [
                '{step_key}_task = DagsterOperator(step=\'{step_key}\', dag=dag, '
                'image=\'{image}\', task_id=\'{step_key}\', s3_conn_id=S3_CONN_ID)'.format(
                    step_key=normalize_key(step.key), image=image
                )
                for step in execution_plan.topological_steps()
            ]
        ),
        step_dependencies='\n'.join(
            [
                '{prev_step_key}_task.set_downstream({step_key}_task)'.format(
                    prev_step_key=normalize_key(step_input.prev_output_handle.step.key),
                    step_key=normalize_key(step.key),
                )
                for step in execution_plan.topological_steps()
                for step_input in step.step_inputs
            ]
        ),
    )

    formatted_dag_file = (
        format_file_contents(dag_file, **BLACK_KWARGS) if BLACK else FormatCode(dag_file)[0]
    )

    if output_path:
        with open(output_path, 'w') as fd:
            fd.write(formatted_dag_file)

    return formatted_dag_file


## NEED TO CODEGEN TWO FILES
# TODO
# - Need to figure out how to scaffold an S3 hook so that the DagsterOperator has access to S3 for
#   persisting outputs
# - Need to figure out how to scaffold a Docker hook so that we can use a custom registry
