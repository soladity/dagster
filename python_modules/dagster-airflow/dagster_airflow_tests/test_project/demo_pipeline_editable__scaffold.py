'''Editable scaffolding autogenerated by dagster-airflow from pipeline demo_pipeline with config:

    context:
      default:
        config: {log_level: DEBUG}
    solids:
      multiply_the_word:
        config: {factor: 2}
        inputs:
          word: {value: bar}
    

By convention, users should attempt to isolate post-codegen changes and customizations to this
"editable" file, rather than changing the definitions in the "static"
demo_pipeline_static__scaffold.py file. Please let us know if you are encountering use cases where
it is necessary to make changes to the static file.
'''

import datetime

from demo_pipeline_static__scaffold import make_dag

# Arguments to be passed to the ``default_args`` parameter of the ``airflow.DAG`` constructor.You
# can override these with values of your choice.
DEFAULT_ARGS = {
    'depends_on_past': False,
    'email': ['airflow@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'owner': 'airflow',
    'retries': 1,
    'retry_delay': datetime.timedelta(0, 300),
    'start_date': datetime.datetime(1900, 1, 1, 0, 0),
}

# Any additional keyword arguments to be passed to the ``airflow.DAG`` constructor. You can override
# these with values of your choice.
DAG_KWARGS = {
    'schedule_interval': '0 0 * * *',
}

# The name of the autogenerated DAG. By default, this is just the name of the Dagster pipeline from
# which the Airflow DAG was generated (demo_pipeline). You may want to override this if, for
# instance, you want to schedule multiple DAGs corresponding to different configurations of the same
# Dagster pipeline.
DAG_ID = 'demo_pipeline'

# The description of the autogenerated DAG. By default, this is the description of the Dagster
# pipeline from which the Airflow DAG was generated. You may want to override this, as with the
# DAG_ID parameter.
DAG_DESCRIPTION = '''***Autogenerated by dagster-airflow***'''

# Additional arguments, if any, to pass to the underlying
# ``dagster_airflow.dagster_plugin.ModifiedDockerOperator`` constructor. Set these if, for instance,
# you need to set special TLS parameters.
MODIFIED_DOCKER_OPERATOR_KWARGS = {}

# Set your S3 connection id here, if you do not want to use the default ``aws_default`` connection.
S3_CONN_ID = 'aws_default'

# Set the host directory to mount into /tmp/results on the containers.
HOST_TMP_DIR = '/tmp/results'

# The 'unusual_prefix' ensures that the following code will be executed only when
# Airflow imports this file. See: https://bcb.github.io/airflow/hide-globals-in-dag-definition-file
if __name__.startswith('unusual_prefix'):
    dag, tasks = make_dag(
        dag_id=DAG_ID,
        dag_description=DAG_DESCRIPTION,
        dag_kwargs=dict(default_args=DEFAULT_ARGS, **DAG_KWARGS),
        s3_conn_id=S3_CONN_ID,
        modified_docker_operator_kwargs=MODIFIED_DOCKER_OPERATOR_KWARGS,
        host_tmp_dir=HOST_TMP_DIR,
    )
