import copy
import json
import logging
import sqlite3
import sys
import threading
import time
from contextlib import contextmanager

from dagster import check, seven
from dagster.core.log_manager import DagsterLogManager

from .event_sink import EventSink

CREATE_LOG_TABLE_STATEMENT = '''create table if not exists logs (
    timestamp integer primary key asc,
    json_log text
)
'''

INSERT_LOG_RECORD_STATEMENT = '''insert into logs (json_log) values (?)'''

RETRIEVE_LOG_RECORDS_STATEMENT = '''select * from logs
    where timestamp >= ?
    order by timestamp asc
'''

if sys.version_info < (3,):  # pragma: nocover
    EVENT_TYPE = threading._Event  # pylint: disable=no-member, protected-access
else:
    EVENT_TYPE = threading.Event


class _LogWatcher(object):
    def __init__(self, sqlite_db_path, log_manager, is_done):
        check.str_param(sqlite_db_path, 'sqlite_db_path')
        check.inst_param(log_manager, 'log_manager', DagsterLogManager)
        check.inst_param(is_done, 'is_done', EVENT_TYPE)

        self.sqlite_db_path = sqlite_db_path
        self.next_timestamp = 0
        self.log_manager = log_manager
        self.is_done = is_done
        self.conn = None

    def connect(self):
        if self.conn is None:
            self.conn = sqlite3.connect(self.sqlite_db_path)
        return self.conn

    def watch(self):
        from dagster.core.serdes import unpack_value

        last_pass = False
        while True:
            if self.is_done.is_set():
                last_pass = True
            else:
                time.sleep(0.1)  # 100 ms

            with self.connect() as conn:
                res = (
                    conn.cursor()
                    .execute(RETRIEVE_LOG_RECORDS_STATEMENT, (self.next_timestamp,))
                    .fetchall()
                )

                if res:
                    self.next_timestamp = res[-1][0] + 1
                    json_records = [r[1] for r in res]
                    for json_record in json_records:
                        base_dict = json.loads(json_record)

                        if base_dict.get('dagster_meta'):
                            base_dict['dagster_meta'] = unpack_value(base_dict['dagster_meta'])

                        record = logging.makeLogRecord(base_dict)

                        for logger in self.log_manager.loggers:
                            for handler in logger.handlers:
                                # Because we're rehydrating the LogMessage, rather than passing
                                # through Logger._log again (which would obscure the original metadata)
                                # we need to filter for log level here
                                if handler.level <= record.levelno:
                                    handler.handle(record)

            if last_pass:
                conn.close()
                return


class SqliteEventSink(EventSink):
    '''
    The multiprocess executor processses execution steps in child processes and recieves back the system
    events. Dagstermill notebooks are executed in a subprocess (opened by jupyter_client.manager.KernelManager).

    Given these setups, we need a way to ship logs from the subprocess back to the parent process
    (executing the solid compute logic), but since we don't manage the subprocess directly in some cases, we can't use a
    Queue/QueueHandler-based scheme. (Socket-based schemes might work, but getting these to play nicely with
    Docker and the wide range of possible user networking setups sounds like a headache.)

    There are a variety of persistent file-based queue projects in the Python ecosystem, but they are
    written for multithreaded cases, not multiprocessing cases; Celery is too heavy for our use
    case; and obviously we want to avoid implementing our own locking logic (perhaps on top of
    filelock).

    Instead we use a sqlite database as a queue. The elements of this setup are exhibited cleanly
    in dagster_tests/core/test_sqlite_event_sink.py

    1. Parent creates a SqliteEventSink. It will init a db, in which we rely on a
    sqlite autoincrement primary key to preserve the order of log messages.
    2. Parent process calls log_forwarding on the event sink as a context manager. This will manage
    a watcher thread that spins up a JsonSqlite3LogWatcher, which polls the
    sqlite db table for new logs until its is_done flag (a threading.Event) is set by closing
    the context manager. New log messages are rehydrated using logging.makeLogRecord and handed
    directly to the logging.Handlers configured on the pipeline DagsterLogManager.
    3. Child process is started to execute using the SqliteEventSink via RunConfig. The sink writes
    the internal dict representation of logging.LogRecords generated by the dagster logging
    machinery to the sqlite table as json.
    4. Child process returns, parent process closes context manager which waits for watcher
    process to complete logging.
    '''

    def __init__(self, sqlite_path, skip_db_init=False, raise_on_error=False):
        super(SqliteEventSink, self).__init__(raise_on_error=raise_on_error)
        self.conn = None
        self.sqlite_path = sqlite_path

        if not skip_db_init:
            with sqlite3.connect(sqlite_path) as con:
                con.execute(CREATE_LOG_TABLE_STATEMENT)
            con.close()

    def on_raw_log_record(self, record):
        from dagster.core.serdes import pack_value

        log_dict = copy.copy(record.__dict__)
        if log_dict.get('dagster_meta'):
            log_dict['dagster_meta'] = pack_value(log_dict['dagster_meta'])

        with self.connect() as con:
            con.execute(INSERT_LOG_RECORD_STATEMENT, (seven.json.dumps(log_dict),))

    def connect(self):
        if self.conn is None:
            self.conn = sqlite3.connect(self.sqlite_path)
        return self.conn

    def on_pipeline_teardown(self):
        if self.conn:
            self.conn.close()

    @contextmanager
    def log_forwarding(self, log_manager):
        '''
        This context manager will take care of a thread to poll the sqlite file, read the
        recorded log record entries and forward them back through to the provided
        DagsterLogManager.
        '''
        check.inst_param(log_manager, 'log_manager', DagsterLogManager)

        # Although the type of is_done is threading._Event in py2, not threading.Event,
        # it is still constructed using the threading.Event() factory
        done_event = threading.Event()

        def log_watcher_thread_target():
            log_watcher = _LogWatcher(self.sqlite_path, log_manager, done_event)
            log_watcher.watch()

        watcher_thread = threading.Thread(target=log_watcher_thread_target)
        watcher_thread.start()

        try:
            yield
        finally:
            done_event.set()
            watcher_thread.join()
